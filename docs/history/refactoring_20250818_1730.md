# Refactoring Plan: Phase 2 - WebView ê¸°ë°˜ ì›¹ í¬ë¡¤ëŸ¬ MCP ë„êµ¬ êµ¬í˜„

## ì‘ì—…ì˜ ëª©ì 

SynapticFlowì— Tauri WebViewë¥¼ í™œìš©í•œ ê³ ê¸‰ ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ì¶”ê°€í•˜ì—¬ AI ì—ì´ì „íŠ¸ê°€ JavaScript ê¸°ë°˜ ë™ì  ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ SPA(Single Page Application)ë‚˜ ë¹„ë™ê¸° ë¡œë”© ì½˜í…ì¸ ê¹Œì§€ ì™„ë²½í•˜ê²Œ í¬ë¡¤ë§í•  ìˆ˜ ìˆëŠ” ì°¨ë³„í™”ëœ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

### ì£¼ìš” ëª©í‘œ

1. **WebView ê¸°ë°˜ í¬ë¡¤ëŸ¬**: ì‹¤ì œ ë¸Œë¼ìš°ì € í™˜ê²½ì—ì„œ í˜ì´ì§€ ë Œë”ë§ ë° ë°ì´í„° ì¶”ì¶œ
2. **ë™ì  ì½˜í…ì¸  ì§€ì›**: JavaScriptë¡œ ìƒì„±ë˜ëŠ” ì½˜í…ì¸  ì™„ë²½ ì²˜ë¦¬
3. **ê²°ê³¼ ì €ì¥ ì‹œìŠ¤í…œ**: í¬ë¡¤ë§ ê²°ê³¼ë¥¼ HTML íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œ ì œê³µ
4. **ë‹¤ì–‘í•œ ì¶”ì¶œ ì˜µì…˜**: CSS ì…€ë ‰í„°, ì»¤ìŠ¤í…€ ìŠ¤í¬ë¦½íŠ¸, ìŠ¤í¬ë¦°ìƒ· ê¸°ëŠ¥
5. **ì•ˆì „ì„± ë³´ì¥**: íƒ€ì„ì•„ì›ƒ, ë¦¬ì†ŒìŠ¤ ì œí•œ, ì—ëŸ¬ ì²˜ë¦¬

## í˜„ì¬ì˜ ìƒíƒœ / ë¬¸ì œì 

### ê¸°ì¡´ í¬ë¡¤ë§ í•œê³„ì 

- **ì •ì  ì½˜í…ì¸ ë§Œ ì²˜ë¦¬**: HTTP ìš”ì²­ ê¸°ë°˜ìœ¼ë¡œëŠ” JavaScript ë Œë”ë§ í›„ ì½˜í…ì¸  ì ‘ê·¼ ë¶ˆê°€
- **SPA ì§€ì› ë¶€ì¡±**: React, Vue, Angular ë“± ëª¨ë˜ ì›¹ ì•±ì˜ ë™ì  ì½˜í…ì¸  í¬ë¡¤ë§ ì œí•œ
- **í¬ë¡¤ë§ ë°©ì§€ ìš°íšŒ ì–´ë ¤ì›€**: ì‹¤ì œ ë¸Œë¼ìš°ì € í™˜ê²½ì´ ì•„ë‹ˆì–´ì„œ ê°ì§€ë˜ê¸° ì‰¬ì›€
- **ë³µì¡í•œ ìƒí˜¸ì‘ìš© ë¶ˆê°€**: ë¡œê·¸ì¸, í¼ ì œì¶œ, í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ ë“± ì œí•œ

### í˜„ì¬ MCP ë„êµ¬ í•œê³„

- FilesystemServer: ë¡œì»¬ íŒŒì¼ë§Œ ì²˜ë¦¬ ê°€ëŠ¥
- SandboxServer: ì½”ë“œ ì‹¤í–‰ë§Œ ì§€ì›, ì›¹ ë°ì´í„° ìˆ˜ì§‘ ë¶ˆê°€
- ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ ì „ë¬´

## ë³€ê²½ ì´í›„ì˜ ìƒíƒœ / í•´ê²° íŒì • ê¸°ì¤€

### ì„±ê³µ ê¸°ì¤€

1. **WebView í¬ë¡¤ëŸ¬ ë„êµ¬ ì¶”ê°€**:
   - `crawl_page`: CSS ì…€ë ‰í„°ë¡œ ë°ì´í„° ì¶”ì¶œ
   - `screenshot`: ì›¹í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜
   - `extract_data`: ì»¤ìŠ¤í…€ JavaScript ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

2. **ë™ì  ì½˜í…ì¸  ì™„ë²½ ì²˜ë¦¬**:
   - JavaScript ë Œë”ë§ ì™„ë£Œ í›„ ë°ì´í„° ì¶”ì¶œ
   - ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì™„ë£Œ ëŒ€ê¸° (`networkidle` ì˜µì…˜)
   - AJAX/fetch ê¸°ë°˜ ë¹„ë™ê¸° ì½˜í…ì¸  ì²˜ë¦¬

3. **ê²°ê³¼ ì €ì¥ ë° ê´€ë¦¬**:
   - í¬ë¡¤ë§ëœ HTMLì„ ì•± ìºì‹œ ë””ë ‰í„°ë¦¬ì— í•´ì‹œ ê¸°ë°˜ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
   - ë©”íƒ€ë°ì´í„° í¬í•¨ (URL, ì‹œê°„, ì¶”ì¶œ ë°ì´í„°)
   - íŒŒì¼ ê²½ë¡œë¥¼ ë„êµ¬ ê²°ê³¼ì— í¬í•¨í•˜ì—¬ í›„ì† ì°¸ì¡° ê°€ëŠ¥

4. **ì•ˆì „ì„± ë° ì„±ëŠ¥**:
   - ìµœëŒ€ íƒ€ì„ì•„ì›ƒ 60ì´ˆ ì œí•œ
   - ë°±ê·¸ë¼ìš´ë“œ WebViewë¡œ UI ì˜í–¥ ìµœì†Œí™”
   - í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìë™ ì •ë¦¬

## ìˆ˜ì •ì´ í•„ìš”í•œ ì½”ë“œ ë° ìˆ˜ì •ë¶€ë¶„ì˜ ì½”ë“œ ìŠ¤ë‹ˆí•

### 1. ìƒˆ íŒŒì¼ ìƒì„±: WebView í¬ë¡¤ëŸ¬ ì„œë²„

**íŒŒì¼**: `src-tauri/src/mcp/builtin/webview_crawler.rs`

#### ì™„ì „íˆ ìƒˆë¡œìš´ íŒŒì¼:

```rust
use crate::mcp::server::{BuiltinMCPServer, MCPTool, MCPResponse, MCPError};
use log::{debug, error, info, warn};
use serde_json::{json, Value};
use std::collections::HashMap;
use std::sync::Arc;
use tauri::{AppHandle, Manager, WebviewBuilder, WebviewUrl, WebviewWindow};
use tokio::sync::{oneshot, Mutex};
use uuid::Uuid;
use sha2::{Sha256, Digest};
use std::path::PathBuf;

const MAX_CRAWL_TIMEOUT: u64 = 60;

pub struct WebViewCrawlerServer {
    app_handle: AppHandle,
    active_crawlers: Arc<Mutex<HashMap<String, CrawlerInstance>>>,
}

struct CrawlerInstance {
    webview: WebviewWindow,
    _created_at: std::time::Instant,
}

#[derive(Debug, Clone)]
struct CrawlResult {
    html_content: String,
    extracted_data: serde_json::Value,
    saved_path: Option<PathBuf>,
}

impl WebViewCrawlerServer {
    pub fn new(app_handle: AppHandle) -> Self {
        Self {
            app_handle,
            active_crawlers: Arc::new(Mutex::new(HashMap::new())),
        }
    }

    /// Generate unique hash for the crawled content
    fn generate_content_hash(url: &str, content: &str) -> String {
        let mut hasher = Sha256::new();
        hasher.update(url.as_bytes());
        hasher.update(content.as_bytes());
        hasher.update(chrono::Utc::now().to_rfc3339().as_bytes());
        format!("{:x}", hasher.finalize())[..16].to_string()
    }

    /// Get app temp directory for saving crawl results
    async fn get_crawl_temp_dir(&self) -> Result<PathBuf, String> {
        let app_dir = self.app_handle
            .path_resolver()
            .app_cache_dir()
            .or_else(|| self.app_handle.path_resolver().app_data_dir())
            .ok_or("Failed to get app directory")?;

        let crawl_dir = app_dir.join("crawl_cache");

        if !crawl_dir.exists() {
            tokio::fs::create_dir_all(&crawl_dir)
                .await
                .map_err(|e| format!("Failed to create crawl directory: {}", e))?;
        }

        Ok(crawl_dir)
    }

    /// Save crawled HTML content to temp file
    async fn save_crawl_result(
        &self,
        url: &str,
        html_content: &str,
        extracted_data: &serde_json::Value,
    ) -> Result<PathBuf, String> {
        let crawl_dir = self.get_crawl_temp_dir().await?;
        let content_hash = Self::generate_content_hash(url, html_content);
        let file_name = format!("{}.html", content_hash);
        let file_path = crawl_dir.join(file_name);

        let enhanced_html = format!(
            r#"<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>SynapticFlow Crawl Result</title>
    <style>
        .synaptic-metadata {{
            background: #f5f5f5;
            border: 1px solid #ddd;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
            font-family: monospace;
        }}
        .synaptic-original {{
            border-top: 2px solid #007acc;
            margin-top: 20px;
        }}
    </style>
</head>
<body>
    <div class="synaptic-metadata">
        <h2>ğŸ•·ï¸ SynapticFlow Crawl Metadata</h2>
        <p><strong>URL:</strong> {}</p>
        <p><strong>Crawled at:</strong> {}</p>
        <p><strong>Content Hash:</strong> {}</p>
        <details>
            <summary><strong>Extracted Data</strong></summary>
            <pre>{}</pre>
        </details>
    </div>

    <div class="synaptic-original">
        <h2>ğŸ“„ Original Content</h2>
        {}
    </div>
</body>
</html>"#,
            html_escape::encode_text(url),
            chrono::Utc::now().to_rfc3339(),
            content_hash,
            serde_json::to_string_pretty(extracted_data).unwrap_or_default(),
            html_content
        );

        tokio::fs::write(&file_path, enhanced_html)
            .await
            .map_err(|e| format!("Failed to save crawl result: {}", e))?;

        info!("Saved crawl result to: {:?}", file_path);
        Ok(file_path)
    }

    // ... ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ (create_hidden_webview, crawl_page, perform_crawl ë“±)
}

#[async_trait::async_trait]
impl BuiltinMCPServer for WebViewCrawlerServer {
    fn name(&self) -> &str {
        "builtin.webview_crawler"
    }

    fn description(&self) -> &str {
        "Built-in WebView-based web crawler server"
    }

    fn tools(&self) -> Vec<MCPTool> {
        vec![
            self.create_crawl_page_tool(),
            self.create_screenshot_tool(),
            self.create_extract_data_tool(),
        ]
    }

    async fn call_tool(&self, tool_name: &str, args: Value) -> MCPResponse {
        match tool_name {
            "crawl_page" => self.handle_crawl_page(args).await,
            "screenshot" => self.handle_screenshot(args).await,
            "extract_data" => self.handle_extract_data(args).await,
            _ => {
                let request_id = Value::String(Uuid::new_v4().to_string());
                MCPResponse {
                    jsonrpc: "2.0".to_string(),
                    id: Some(request_id),
                    result: None,
                    error: Some(MCPError {
                        code: -32601,
                        message: format!("Tool '{}' not found in webview crawler server", tool_name),
                        data: None,
                    }),
                }
            }
        }
    }
}
```

### 2. Cargo.toml ì˜ì¡´ì„± ì¶”ê°€

**íŒŒì¼**: `src-tauri/Cargo.toml`

#### ì¶”ê°€í•  ì˜ì¡´ì„±:

```toml
[dependencies]
# ê¸°ì¡´ ì˜ì¡´ì„±ë“¤...
sha2 = "0.10"
chrono = { version = "0.4", features = ["serde"] }
html-escape = "0.2"
async-trait = "0.1"
```

### 3. ë¹ŒíŠ¸ì¸ ì„œë²„ ëª¨ë“ˆ ì—…ë°ì´íŠ¸

**íŒŒì¼**: `src-tauri/src/mcp/builtin/mod.rs`

#### í˜„ì¬ ì½”ë“œ:

```rust
pub mod filesystem;
pub mod sandbox;
```

#### ìˆ˜ì • í›„ ì½”ë“œ:

```rust
pub mod filesystem;
pub mod sandbox;
pub mod webview_crawler;

use crate::mcp::server::BuiltinMCPServer;
use std::sync::Arc;
use tauri::AppHandle;

pub fn create_builtin_servers(app_handle: AppHandle) -> Vec<Arc<dyn BuiltinMCPServer + Send + Sync>> {
    vec![
        Arc::new(filesystem::FilesystemServer::new()),
        Arc::new(sandbox::SandboxServer::new()),
        Arc::new(webview_crawler::WebViewCrawlerServer::new(app_handle)),
    ]
}
```

### 4. MCP ì„œë²„ ë§¤ë‹ˆì € ì—…ë°ì´íŠ¸

**íŒŒì¼**: `src-tauri/src/mcp/manager.rs`

#### í˜„ì¬ ì½”ë“œ (create_builtin_servers í˜¸ì¶œ ë¶€ë¶„):

```rust
fn create_builtin_servers(&self) -> HashMap<String, Arc<dyn BuiltinMCPServer + Send + Sync>> {
    let mut servers = HashMap::new();

    let builtin_servers = builtin::create_builtin_servers();
    for server in builtin_servers {
        servers.insert(server.name().to_string(), server);
    }

    servers
}
```

#### ìˆ˜ì • í›„ ì½”ë“œ:

```rust
fn create_builtin_servers(&self) -> HashMap<String, Arc<dyn BuiltinMCPServer + Send + Sync>> {
    let mut servers = HashMap::new();

    if let Some(app_handle) = self.get_app_handle() {
        let builtin_servers = builtin::create_builtin_servers(app_handle.clone());
        for server in builtin_servers {
            servers.insert(server.name().to_string(), server);
        }
    }

    servers
}
```

### 5. AppHandle ì§€ì›ì„ ìœ„í•œ ë§¤ë‹ˆì € í™•ì¥

**íŒŒì¼**: `src-tauri/src/mcp/manager.rs`

#### ì¶”ê°€í•  í•„ë“œ ë° ë©”ì„œë“œ:

```rust
use std::sync::OnceLock;
use tauri::AppHandle;

pub struct MCPServerManager {
    // ê¸°ì¡´ í•„ë“œë“¤...
    app_handle: OnceLock<AppHandle>,
}

impl MCPServerManager {
    pub fn new() -> Self {
        Self {
            // ê¸°ì¡´ ì´ˆê¸°í™”...
            app_handle: OnceLock::new(),
        }
    }

    pub fn set_app_handle(&self, handle: AppHandle) {
        let _ = self.app_handle.set(handle);
    }

    fn get_app_handle(&self) -> Option<&AppHandle> {
        self.app_handle.get()
    }
}
```

### 6. lib.rsì—ì„œ AppHandle ì„¤ì •

**íŒŒì¼**: `src-tauri/src/lib.rs`

#### í˜„ì¬ ì½”ë“œ (run í•¨ìˆ˜ ë‚´ setup):

```rust
#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_log::Builder::default()
            .targets([
                Target::new(TargetKind::Stdout),
                Target::new(TargetKind::LogDir { file_name: None }),
                Target::new(TargetKind::Webview),
            ])
            .build())
        .invoke_handler(tauri::generate_handler![
            greet,
            start_mcp_server,
            stop_mcp_server,
            call_mcp_tool,
            sample_from_mcp_server,
            list_mcp_tools,
            list_tools_from_config,
            get_connected_servers,
            check_server_status,
            check_all_servers_status
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
```

#### ìˆ˜ì • í›„ ì½”ë“œ:

```rust
#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_log::Builder::default()
            .targets([
                Target::new(TargetKind::Stdout),
                Target::new(TargetKind::LogDir { file_name: None }),
                Target::new(TargetKind::Webview),
            ])
            .build())
        .setup(|app| {
            let app_handle = app.handle();
            get_mcp_manager().set_app_handle(app_handle);
            Ok(())
        })
        .invoke_handler(tauri::generate_handler![
            greet,
            start_mcp_server,
            stop_mcp_server,
            call_mcp_tool,
            sample_from_mcp_server,
            list_mcp_tools,
            list_tools_from_config,
            get_connected_servers,
            check_server_status,
            check_all_servers_status
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
```

### 7. BuiltInToolContext.tsx ì—…ë°ì´íŠ¸ (ì„ íƒì‚¬í•­)

**íŒŒì¼**: `src/context/BuiltInToolContext.tsx`

#### í˜„ì¬ ë‚´ì¥ ë„êµ¬ ëª©ë¡ì— ì¶”ê°€:

```typescript
const BUILTIN_TOOLS = [
  // ê¸°ì¡´ ë„êµ¬ë“¤...
  'builtin.webview_crawler',
] as const;

// WebView í¬ë¡¤ëŸ¬ ë„êµ¬ íƒ€ì… ì¶”ê°€
type WebViewCrawlerTool = 'crawl_page' | 'screenshot' | 'extract_data';
```

## ì˜ˆìƒ ê²°ê³¼

### ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” MCP ë„êµ¬ë“¤

1. **crawl_page**:

   ```json
   {
     "url": "https://example.com",
     "selectors": ["h1", ".content", "#main"],
     "wait_for": "networkidle",
     "timeout": 30,
     "save_html": true
   }
   ```

2. **screenshot**:

   ```json
   {
     "url": "https://example.com",
     "width": 1920,
     "height": 1080
   }
   ```

3. **extract_data**:
   ```json
   {
     "url": "https://example.com",
     "script": "return document.querySelectorAll('h1').length;"
   }
   ```

### ì˜ˆìƒ ì‘ë‹µ í˜•ì‹

```json
{
  "content": [
    {
      "type": "text",
      "text": "âœ… Successfully crawled: https://example.com\nğŸ“„ Extracted 3 selectors\nğŸ’¾ Saved HTML to: /Users/app/cache/crawl_cache/a1b2c3d4.html"
    }
  ],
  "data": {
    "url": "https://example.com",
    "extracted_data": {
      "h1": [{ "tagName": "H1", "textContent": "Welcome" }],
      ".content": [{ "tagName": "DIV", "textContent": "Main content" }]
    },
    "selectors_used": ["h1", ".content", "#main"],
    "crawled_at": "2025-08-18T17:00:00Z",
    "saved_html_path": "/Users/app/cache/crawl_cache/a1b2c3d4.html"
  }
}
```

ì´ êµ¬í˜„ì„ í†µí•´ SynapticFlowëŠ” ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ì›¹ í¬ë¡¤ë§ ê¸°ëŠ¥ì„ ê°–ì¶˜ AI ì—ì´ì „íŠ¸ í”Œë«í¼ìœ¼ë¡œ ì°¨ë³„í™”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
