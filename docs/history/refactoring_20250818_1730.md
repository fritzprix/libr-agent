# Refactoring Plan: Phase 2 - WebView 기반 웹 크롤러 MCP 도구 구현

## 작업의 목적

SynapticFlow에 Tauri WebView를 활용한 고급 웹 크롤링 기능을 추가하여 AI 에이전트가 JavaScript 기반 동적 웹사이트에서 실시간 데이터를 수집할 수 있도록 합니다. 이를 통해 SPA(Single Page Application)나 비동기 로딩 콘텐츠까지 완벽하게 크롤링할 수 있는 차별화된 기능을 제공합니다.

### 주요 목표

1. **WebView 기반 크롤러**: 실제 브라우저 환경에서 페이지 렌더링 및 데이터 추출
2. **동적 콘텐츠 지원**: JavaScript로 생성되는 콘텐츠 완벽 처리
3. **결과 저장 시스템**: 크롤링 결과를 HTML 파일로 저장하고 경로 제공
4. **다양한 추출 옵션**: CSS 셀렉터, 커스텀 스크립트, 스크린샷 기능
5. **안전성 보장**: 타임아웃, 리소스 제한, 에러 처리

## 현재의 상태 / 문제점

### 기존 크롤링 한계점

- **정적 콘텐츠만 처리**: HTTP 요청 기반으로는 JavaScript 렌더링 후 콘텐츠 접근 불가
- **SPA 지원 부족**: React, Vue, Angular 등 모던 웹 앱의 동적 콘텐츠 크롤링 제한
- **크롤링 방지 우회 어려움**: 실제 브라우저 환경이 아니어서 감지되기 쉬움
- **복잡한 상호작용 불가**: 로그인, 폼 제출, 페이지 네비게이션 등 제한

### 현재 MCP 도구 한계

- FilesystemServer: 로컬 파일만 처리 가능
- SandboxServer: 코드 실행만 지원, 웹 데이터 수집 불가
- 웹 크롤링 기능 전무

## 변경 이후의 상태 / 해결 판정 기준

### 성공 기준

1. **WebView 크롤러 도구 추가**:
   - `crawl_page`: CSS 셀렉터로 데이터 추출
   - `screenshot`: 웹페이지 스크린샷 캡처
   - `extract_data`: 커스텀 JavaScript 스크립트 실행

2. **동적 콘텐츠 완벽 처리**:
   - JavaScript 렌더링 완료 후 데이터 추출
   - 네트워크 요청 완료 대기 (`networkidle` 옵션)
   - AJAX/fetch 기반 비동기 콘텐츠 처리

3. **결과 저장 및 관리**:
   - 크롤링된 HTML을 앱 캐시 디렉터리에 해시 기반 파일명으로 저장
   - 메타데이터 포함 (URL, 시간, 추출 데이터)
   - 파일 경로를 도구 결과에 포함하여 후속 참조 가능

4. **안전성 및 성능**:
   - 최대 타임아웃 60초 제한
   - 백그라운드 WebView로 UI 영향 최소화
   - 크롤러 인스턴스 자동 정리

## 수정이 필요한 코드 및 수정부분의 코드 스니핏

### 1. 새 파일 생성: WebView 크롤러 서버

**파일**: `src-tauri/src/mcp/builtin/webview_crawler.rs`

#### 완전히 새로운 파일:

```rust
use crate::mcp::server::{BuiltinMCPServer, MCPTool, MCPResponse, MCPError};
use log::{debug, error, info, warn};
use serde_json::{json, Value};
use std::collections::HashMap;
use std::sync::Arc;
use tauri::{AppHandle, Manager, WebviewBuilder, WebviewUrl, WebviewWindow};
use tokio::sync::{oneshot, Mutex};
use uuid::Uuid;
use sha2::{Sha256, Digest};
use std::path::PathBuf;

const MAX_CRAWL_TIMEOUT: u64 = 60;

pub struct WebViewCrawlerServer {
    app_handle: AppHandle,
    active_crawlers: Arc<Mutex<HashMap<String, CrawlerInstance>>>,
}

struct CrawlerInstance {
    webview: WebviewWindow,
    _created_at: std::time::Instant,
}

#[derive(Debug, Clone)]
struct CrawlResult {
    html_content: String,
    extracted_data: serde_json::Value,
    saved_path: Option<PathBuf>,
}

impl WebViewCrawlerServer {
    pub fn new(app_handle: AppHandle) -> Self {
        Self {
            app_handle,
            active_crawlers: Arc::new(Mutex::new(HashMap::new())),
        }
    }

    /// Generate unique hash for the crawled content
    fn generate_content_hash(url: &str, content: &str) -> String {
        let mut hasher = Sha256::new();
        hasher.update(url.as_bytes());
        hasher.update(content.as_bytes());
        hasher.update(chrono::Utc::now().to_rfc3339().as_bytes());
        format!("{:x}", hasher.finalize())[..16].to_string()
    }

    /// Get app temp directory for saving crawl results
    async fn get_crawl_temp_dir(&self) -> Result<PathBuf, String> {
        let app_dir = self.app_handle
            .path_resolver()
            .app_cache_dir()
            .or_else(|| self.app_handle.path_resolver().app_data_dir())
            .ok_or("Failed to get app directory")?;

        let crawl_dir = app_dir.join("crawl_cache");

        if !crawl_dir.exists() {
            tokio::fs::create_dir_all(&crawl_dir)
                .await
                .map_err(|e| format!("Failed to create crawl directory: {}", e))?;
        }

        Ok(crawl_dir)
    }

    /// Save crawled HTML content to temp file
    async fn save_crawl_result(
        &self,
        url: &str,
        html_content: &str,
        extracted_data: &serde_json::Value,
    ) -> Result<PathBuf, String> {
        let crawl_dir = self.get_crawl_temp_dir().await?;
        let content_hash = Self::generate_content_hash(url, html_content);
        let file_name = format!("{}.html", content_hash);
        let file_path = crawl_dir.join(file_name);

        let enhanced_html = format!(
            r#"<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>SynapticFlow Crawl Result</title>
    <style>
        .synaptic-metadata {{
            background: #f5f5f5;
            border: 1px solid #ddd;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
            font-family: monospace;
        }}
        .synaptic-original {{
            border-top: 2px solid #007acc;
            margin-top: 20px;
        }}
    </style>
</head>
<body>
    <div class="synaptic-metadata">
        <h2>🕷️ SynapticFlow Crawl Metadata</h2>
        <p><strong>URL:</strong> {}</p>
        <p><strong>Crawled at:</strong> {}</p>
        <p><strong>Content Hash:</strong> {}</p>
        <details>
            <summary><strong>Extracted Data</strong></summary>
            <pre>{}</pre>
        </details>
    </div>

    <div class="synaptic-original">
        <h2>📄 Original Content</h2>
        {}
    </div>
</body>
</html>"#,
            html_escape::encode_text(url),
            chrono::Utc::now().to_rfc3339(),
            content_hash,
            serde_json::to_string_pretty(extracted_data).unwrap_or_default(),
            html_content
        );

        tokio::fs::write(&file_path, enhanced_html)
            .await
            .map_err(|e| format!("Failed to save crawl result: {}", e))?;

        info!("Saved crawl result to: {:?}", file_path);
        Ok(file_path)
    }

    // ... 나머지 메서드들 (create_hidden_webview, crawl_page, perform_crawl 등)
}

#[async_trait::async_trait]
impl BuiltinMCPServer for WebViewCrawlerServer {
    fn name(&self) -> &str {
        "builtin.webview_crawler"
    }

    fn description(&self) -> &str {
        "Built-in WebView-based web crawler server"
    }

    fn tools(&self) -> Vec<MCPTool> {
        vec![
            self.create_crawl_page_tool(),
            self.create_screenshot_tool(),
            self.create_extract_data_tool(),
        ]
    }

    async fn call_tool(&self, tool_name: &str, args: Value) -> MCPResponse {
        match tool_name {
            "crawl_page" => self.handle_crawl_page(args).await,
            "screenshot" => self.handle_screenshot(args).await,
            "extract_data" => self.handle_extract_data(args).await,
            _ => {
                let request_id = Value::String(Uuid::new_v4().to_string());
                MCPResponse {
                    jsonrpc: "2.0".to_string(),
                    id: Some(request_id),
                    result: None,
                    error: Some(MCPError {
                        code: -32601,
                        message: format!("Tool '{}' not found in webview crawler server", tool_name),
                        data: None,
                    }),
                }
            }
        }
    }
}
```

### 2. Cargo.toml 의존성 추가

**파일**: `src-tauri/Cargo.toml`

#### 추가할 의존성:

```toml
[dependencies]
# 기존 의존성들...
sha2 = "0.10"
chrono = { version = "0.4", features = ["serde"] }
html-escape = "0.2"
async-trait = "0.1"
```

### 3. 빌트인 서버 모듈 업데이트

**파일**: `src-tauri/src/mcp/builtin/mod.rs`

#### 현재 코드:

```rust
pub mod filesystem;
pub mod sandbox;
```

#### 수정 후 코드:

```rust
pub mod filesystem;
pub mod sandbox;
pub mod webview_crawler;

use crate::mcp::server::BuiltinMCPServer;
use std::sync::Arc;
use tauri::AppHandle;

pub fn create_builtin_servers(app_handle: AppHandle) -> Vec<Arc<dyn BuiltinMCPServer + Send + Sync>> {
    vec![
        Arc::new(filesystem::FilesystemServer::new()),
        Arc::new(sandbox::SandboxServer::new()),
        Arc::new(webview_crawler::WebViewCrawlerServer::new(app_handle)),
    ]
}
```

### 4. MCP 서버 매니저 업데이트

**파일**: `src-tauri/src/mcp/manager.rs`

#### 현재 코드 (create_builtin_servers 호출 부분):

```rust
fn create_builtin_servers(&self) -> HashMap<String, Arc<dyn BuiltinMCPServer + Send + Sync>> {
    let mut servers = HashMap::new();

    let builtin_servers = builtin::create_builtin_servers();
    for server in builtin_servers {
        servers.insert(server.name().to_string(), server);
    }

    servers
}
```

#### 수정 후 코드:

```rust
fn create_builtin_servers(&self) -> HashMap<String, Arc<dyn BuiltinMCPServer + Send + Sync>> {
    let mut servers = HashMap::new();

    if let Some(app_handle) = self.get_app_handle() {
        let builtin_servers = builtin::create_builtin_servers(app_handle.clone());
        for server in builtin_servers {
            servers.insert(server.name().to_string(), server);
        }
    }

    servers
}
```

### 5. AppHandle 지원을 위한 매니저 확장

**파일**: `src-tauri/src/mcp/manager.rs`

#### 추가할 필드 및 메서드:

```rust
use std::sync::OnceLock;
use tauri::AppHandle;

pub struct MCPServerManager {
    // 기존 필드들...
    app_handle: OnceLock<AppHandle>,
}

impl MCPServerManager {
    pub fn new() -> Self {
        Self {
            // 기존 초기화...
            app_handle: OnceLock::new(),
        }
    }

    pub fn set_app_handle(&self, handle: AppHandle) {
        let _ = self.app_handle.set(handle);
    }

    fn get_app_handle(&self) -> Option<&AppHandle> {
        self.app_handle.get()
    }
}
```

### 6. lib.rs에서 AppHandle 설정

**파일**: `src-tauri/src/lib.rs`

#### 현재 코드 (run 함수 내 setup):

```rust
#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_log::Builder::default()
            .targets([
                Target::new(TargetKind::Stdout),
                Target::new(TargetKind::LogDir { file_name: None }),
                Target::new(TargetKind::Webview),
            ])
            .build())
        .invoke_handler(tauri::generate_handler![
            greet,
            start_mcp_server,
            stop_mcp_server,
            call_mcp_tool,
            sample_from_mcp_server,
            list_mcp_tools,
            list_tools_from_config,
            get_connected_servers,
            check_server_status,
            check_all_servers_status
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
```

#### 수정 후 코드:

```rust
#[cfg_attr(mobile, tauri::mobile_entry_point)]
pub fn run() {
    tauri::Builder::default()
        .plugin(tauri_plugin_log::Builder::default()
            .targets([
                Target::new(TargetKind::Stdout),
                Target::new(TargetKind::LogDir { file_name: None }),
                Target::new(TargetKind::Webview),
            ])
            .build())
        .setup(|app| {
            let app_handle = app.handle();
            get_mcp_manager().set_app_handle(app_handle);
            Ok(())
        })
        .invoke_handler(tauri::generate_handler![
            greet,
            start_mcp_server,
            stop_mcp_server,
            call_mcp_tool,
            sample_from_mcp_server,
            list_mcp_tools,
            list_tools_from_config,
            get_connected_servers,
            check_server_status,
            check_all_servers_status
        ])
        .run(tauri::generate_context!())
        .expect("error while running tauri application");
}
```

### 7. BuiltInToolContext.tsx 업데이트 (선택사항)

**파일**: `src/context/BuiltInToolContext.tsx`

#### 현재 내장 도구 목록에 추가:

```typescript
const BUILTIN_TOOLS = [
  // 기존 도구들...
  'builtin.webview_crawler',
] as const;

// WebView 크롤러 도구 타입 추가
type WebViewCrawlerTool = 'crawl_page' | 'screenshot' | 'extract_data';
```

## 예상 결과

### 새로 추가되는 MCP 도구들

1. **crawl_page**:

   ```json
   {
     "url": "https://example.com",
     "selectors": ["h1", ".content", "#main"],
     "wait_for": "networkidle",
     "timeout": 30,
     "save_html": true
   }
   ```

2. **screenshot**:

   ```json
   {
     "url": "https://example.com",
     "width": 1920,
     "height": 1080
   }
   ```

3. **extract_data**:
   ```json
   {
     "url": "https://example.com",
     "script": "return document.querySelectorAll('h1').length;"
   }
   ```

### 예상 응답 형식

```json
{
  "content": [
    {
      "type": "text",
      "text": "✅ Successfully crawled: https://example.com\n📄 Extracted 3 selectors\n💾 Saved HTML to: /Users/app/cache/crawl_cache/a1b2c3d4.html"
    }
  ],
  "data": {
    "url": "https://example.com",
    "extracted_data": {
      "h1": [{ "tagName": "H1", "textContent": "Welcome" }],
      ".content": [{ "tagName": "DIV", "textContent": "Main content" }]
    },
    "selectors_used": ["h1", ".content", "#main"],
    "crawled_at": "2025-08-18T17:00:00Z",
    "saved_html_path": "/Users/app/cache/crawl_cache/a1b2c3d4.html"
  }
}
```

이 구현을 통해 SynapticFlow는 업계 최고 수준의 웹 크롤링 기능을 갖춘 AI 에이전트 플랫폼으로 차별화될 수 있습니다.
