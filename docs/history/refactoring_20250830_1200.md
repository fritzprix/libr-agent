# Refactoring: LLM Context Size Enforcement Implementation

## 작업의 목적

LLM의 context window를 초과하지 않도록 메시지 배열을 제한하여, API 호출 시 overflow 오류를 방지하고 안정적인 서비스를 제공한다. UI에서는 모든 메시지를 표시하되, 실제 LLM 전달 시에만 토큰 수를 기반으로 안전하게 잘라낸다.

## 현재의 상태 / 문제점

- 메시지 배열이 LLM의 context window를 초과할 경우 API 오류 발생 가능
- 현재 메시지 선택 로직이 없어, 긴 대화에서 overflow 위험이 있음
- 토큰 수 추정이 없어 정확한 제한이 어려움

## 변경 이후의 상태 / 해결 판정 기준

- LLM 호출 시 메시지 배열이 모델의 context window 90% 이내로 제한됨 (10% 안전 마진 적용)
- UI 렌더링은 모든 메시지를 유지하며 영향 없음
- 토큰 추정은 BPE 기반으로 정확하게 수행
- overflow 오류 없이 LLM API 호출 성공

## 수정이 필요한 코드 및 수정부분의 코드 스니핏

### 1. 새로운 파일: `src/lib/token-utils.ts`

토큰 추정 및 메시지 선택 유틸리티를 구현한다.

```typescript
import { get_encoding } from '@dqbd/tiktoken';
import type { Message } from '@/models/chat';
import { llmConfigManager } from './llm-config-manager';

/**
 * BPE(cl100k_base) 기준으로 메시지의 토큰 수를 추정합니다.
 */
export function estimateTokensBPE(message: Message): number {
  const text = `${message.role}: ${message.content ?? ''}`;
  const encoding = get_encoding('cl100k_base');
  return encoding.encode(text).length;
}

/**
 * 모델의 contextWindow에서 10% 마진을 적용하여, 초과하지 않는 메시지 배열을 반환합니다.
 */
export function selectMessagesWithinContext(
  messages: Message[],
  providerId: string,
  modelId: string,
): Message[] {
  const modelInfo = llmConfigManager.getModel(providerId, modelId);
  if (!modelInfo) return [];

  // 10% 마진 적용
  const safeWindow = Math.floor(modelInfo.contextWindow * 0.9);

  let totalTokens = 0;
  const selected: Message[] = [];
  for (let i = messages.length - 1; i >= 0; i--) {
    const msg = messages[i];
    const tokens = estimateTokensBPE(msg);
    if (totalTokens + tokens > safeWindow) break;
    selected.unshift(msg);
    totalTokens += tokens;
  }
  return selected;
}
```

### 2. 수정 파일: `src/hooks/use-ai-service.ts`

LLM 호출 직전 메시지 배열을 context window에 맞춰 잘라내는 로직을 추가한다.

```typescript
// ...existing code...
import { selectMessagesWithinContext } from '@/lib/token-utils';
// ...existing code...

const submit = useCallback(
  async (
    messages: Message[],
    systemPrompt?: string | (() => Promise<string>),
  ): Promise<Message> => {
    // ...existing code...

    // Context enforcement: 메시지 배열을 context window에 맞춰 잘라냄
    const safeMessages = selectMessagesWithinContext(
      processedMessages,
      provider,
      model,
    );

    logger.info('Submitting messages to AI service', {
      model,
      systemPrompt: resolvedSystemPrompt,
      messageCount: safeMessages.length,  // 잘린 메시지 수 로깅
    });

    const stream = serviceInstance.streamChat(safeMessages, {
      // ...existing code...
    });

    // ...existing code...
  },
  [model, config, serviceInstance],
);
// ...existing code...
```

### 3. 확인 파일: `src/context/SessionHistoryContext.tsx`

변경 없음. 전체 메시지 관리 및 UI 렌더링을 유지한다.

## 추가 고려사항

- `@dqbd/tiktoken` 패키지 설치 필요 (`pnpm add @dqbd/tiktoken`)
- 모델별 인코딩 매핑 없이 cl100k_base 사용으로 대부분의 LLM 호환
- 필요 시 토큰 추정 정확도를 위해 모델별 인코딩 추가 가능
- 테스트 시 실제 LLM API 호출로 overflow 검증 권장
