# File Attachment MCP Server Module êµ¬í˜„ ê³„íš

## ğŸ“‹ ê°œìš”

mcp-plan.mdì˜ íŒŒì¼ ì²¨ë¶€ ì‹œìŠ¤í…œì„ Web Worker MCP ì„œë²„ ëª¨ë“ˆë¡œ êµ¬í˜„í•˜ì—¬, WebMCPContextë¥¼ í†µí•´ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒŒì¼ ê´€ë¦¬ ë° ì˜ë¯¸ì  ê²€ìƒ‰ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ êµ¬í˜„ ëª©í‘œ

- **Web Worker ê¸°ë°˜**: BM25 ìš°ì„  êµ¬í˜„, í–¥í›„ transformers.js ì„ë² ë”©ìœ¼ë¡œ í™•ì¥
- **í‘œì¤€ MCP ì¸í„°í˜ì´ìŠ¤**: ê¸°ì¡´ calculator, filesystem ëª¨ë“ˆê³¼ ë™ì¼í•œ êµ¬ì¡°
- **IndexedDB ì €ì¥ì†Œ**: ê¸°ì¡´ db.ts êµ¬ì¡° í™œìš©í•œ ì˜êµ¬ ì €ì¥
- **ì ì§„ì  ê²€ìƒ‰ í–¥ìƒ**: BM25 â†’ ì„ë² ë”© â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë‹¨ê³„ë³„ í™•ì¥
- **ì¶”ìƒí™” ì„¤ê³„**: ê²€ìƒ‰ ì—”ì§„ êµì²´ ê°€ëŠ¥í•œ ì¸í„°í˜ì´ìŠ¤ êµ¬ì¡°

## âš ï¸ ì£¼ìš” ì£¼ì˜ì‚¬í•­

> **Note: MVP ìš°ì„  ì ‘ê·¼ë²•**
> - 1ë‹¨ê³„: BM25 ê¸°ë°˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ MVP êµ¬í˜„
> - 2ë‹¨ê³„: transformers.js ì„ë² ë”©ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì„ íƒì ìœ¼ë¡œ ë¡œë”©
> - 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(BM25 + ì„ë² ë”©) ì§€ì›

> **Note: ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ì œì•½**
> - Web Worker í™˜ê²½ì—ì„œ 50MB ë©”ëª¨ë¦¬ ì œí•œ ê³ ë ¤
> - ëŒ€ìš©ëŸ‰ íŒŒì¼(10MB+)ì€ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ í•„ìˆ˜
> - transformers.js ëª¨ë¸ ë¡œë”© ì‹œê°„(5-30ì´ˆ) ê³ ë ¤í•œ Progressive Enhancement

> **Note: ë¸Œë¼ìš°ì € í˜¸í™˜ì„±**
> - IndexedDB, Web Worker, WebAssembly ì§€ì› í™•ì¸ í•„ìš”
> - transformers.jsì˜ WebGPU ì§€ì›ì€ ì œí•œì  (í´ë°± ì „ëµ í•„ìš”)
> - Cross-Origin ì •ì±…ìœ¼ë¡œ ì¸í•œ ëª¨ë¸ ë¡œë”© ì´ìŠˆ ëŒ€ë¹„

> **Note: ë³´ì•ˆ ê³ ë ¤ì‚¬í•­**
> - íŒŒì¼ í¬ê¸° ì œí•œ (ìµœëŒ€ 50MB)
> - ì•…ì„± ìŠ¤í¬ë¦½íŠ¸ íŒ¨í„´ ê²€ì‚¬
> - MIME íƒ€ì… ê²€ì¦ ë° í—ˆìš© ëª©ë¡ ê´€ë¦¬

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React UI       â”‚    â”‚  WebMCPContext   â”‚    â”‚  Web Worker     â”‚
â”‚  File Upload    â”‚â”€â”€â”€â–¶â”‚  Provider        â”‚â”€â”€â”€â–¶â”‚  file-store.ts  â”‚
â”‚  Search UI      â”‚    â”‚  Hook Integrationâ”‚    â”‚  + BM25/Embeddingâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                                                         â–¼
                                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                               â”‚  IndexedDB      â”‚
                                               â”‚  - Stores       â”‚
                                               â”‚  - Content      â”‚
                                               â”‚  - Chunks       â”‚
                                               â”‚  - Embeddings   â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ê²€ìƒ‰ ì—”ì§„ ì¶”ìƒí™” êµ¬ì¡°

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ISearchEngine     â”‚  â† ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤
â”‚   Interface         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–²
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BM25Engine  â”‚ â”‚EmbeddingEngineâ”‚  â† êµ¬í˜„ì²´ë“¤
â”‚ (MVP)       â”‚ â”‚ (Enhanced)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ íŒŒì¼ êµ¬ì¡° ë° ì½”ë“œ ë°°ì¹˜

### 1. ë°ì´í„°ë² ì´ìŠ¤ í™•ì¥ (ê¸°ì¡´ í™œìš©)
```text
src/lib/db.ts                 # âœ… ê¸°ì¡´ íŒŒì¼ í™•ì¥
â”œâ”€â”€ FileStore, FileContent, FileChunk íƒ€ì… ì¶”ê°€
â”œâ”€â”€ Version 5 ìŠ¤í‚¤ë§ˆ ì¶”ê°€ (fileStores, fileContents, fileChunks)
â”œâ”€â”€ CRUD ì„œë¹„ìŠ¤ ì¶”ê°€ (ê¸°ì¡´ íŒ¨í„´ ë™ì¼)
â””â”€â”€ íŒŒì¼ ì²¨ë¶€ìš© ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì¶”ê°€
```

### 2. MCP Server Module (ìƒˆë¡œ ìƒì„±)
```text
src/lib/web-mcp/modules/
â”œâ”€â”€ calculator.ts             # âœ… ê¸°ì¡´
â”œâ”€â”€ filesystem.ts             # âœ… ê¸°ì¡´
â””â”€â”€ file-store.ts             # ğŸ†• ìƒˆë¡œ êµ¬í˜„
    â”œâ”€â”€ ISearchEngine ì¸í„°í˜ì´ìŠ¤
    â”œâ”€â”€ BM25SearchEngine (MVP)
    â”œâ”€â”€ EmbeddingSearchEngine (í–¥í›„ í™•ì¥)
    â”œâ”€â”€ AdaptiveSearchManager
    â”œâ”€â”€ TextChunker ìœ í‹¸ë¦¬í‹°
    â””â”€â”€ FileStoreServer MCP êµ¬í˜„
```

### 3. íƒ€ì… ì •ì˜ ìœ„ì¹˜ (ê¸°ì¡´ í™œìš©)
```text
src/models/
â”œâ”€â”€ chat.ts                   # âœ… ê¸°ì¡´ - AttachmentReference ì´ë¯¸ ì •ì˜ë¨!
â””â”€â”€ search-engine.ts          # ğŸ†• ìƒˆë¡œ ìƒì„± (ê²€ìƒ‰ ì—”ì§„ ì „ìš©)
    â”œâ”€â”€ ISearchEngine ì¸í„°í˜ì´ìŠ¤
    â”œâ”€â”€ SearchResult, SearchOptions
    â””â”€â”€ BM25/Embedding ê´€ë ¨ íƒ€ì…
```

**âœ… ì¤‘ìš” ë°œê²¬**: `AttachmentReference` íƒ€ì…ì´ `src/models/chat.ts`ì— ì´ë¯¸ ì™„ë²½í•˜ê²Œ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤!
```typescript
// src/models/chat.ts (ê¸°ì¡´)
export interface AttachmentReference {
  storeId: string;         // MCP íŒŒì¼ ì €ì¥ì†Œ ID
  contentId: string;       // MCP ì»¨í…ì¸  ID
  filename: string;        // ì›ë³¸ íŒŒì¼ëª…
  mimeType: string;        // MIME íƒ€ì…
  size: number;            // íŒŒì¼ í¬ê¸° (bytes)
  lineCount: number;       // ì´ ë¼ì¸ ìˆ˜
  preview: string;         // ì²« 10-20ì¤„ ë¯¸ë¦¬ë³´ê¸°
  uploadedAt: string;      // ì—…ë¡œë“œ ì‹œê°„ (ISO 8601)
  chunkCount?: number;     // ì²­í¬ ê°œìˆ˜ (ê²€ìƒ‰ìš©)
  lastAccessedAt?: string; // ë§ˆì§€ë§‰ ì ‘ê·¼ ì‹œê°„
}
```

### 4. Hook ë° Context (ê¸°ì¡´ íŒ¨í„´ í™œìš©)

```text
src/hooks/
â””â”€â”€ use-file-attachment.ts    # ğŸ†• ìƒˆë¡œ ìƒì„±
    â”œâ”€â”€ useFileStore (ìŠ¤í† ì–´ ê´€ë¦¬)
    â”œâ”€â”€ useFileUpload (íŒŒì¼ ì—…ë¡œë“œ) 
    â””â”€â”€ useFileSearch (ê²€ìƒ‰ ê¸°ëŠ¥)
    â€» AttachmentReference íƒ€ì…ì€ ê¸°ì¡´ chat.tsì—ì„œ import

src/context/
â””â”€â”€ WebMCPContext.tsx         # ğŸ”„ ê¸°ì¡´ íŒŒì¼ í™•ì¥
    â”œâ”€â”€ ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€
    â””â”€â”€ íŒŒì¼ ì²¨ë¶€ ê´€ë ¨ ë©”ì„œë“œ ì¶”ê°€
```

### 5. UI ì»´í¬ë„ŒíŠ¸ (ê¸°ì¡´ íŒ¨í„´ í™œìš©)

```text
src/features/file-attachment/  # ğŸ†• ìƒˆë¡œ ìƒì„±
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ FileUpload.tsx         # AttachmentReference í™œìš©
â”‚   â”œâ”€â”€ FileList.tsx           # AttachmentReference[] í‘œì‹œ
â”‚   â”œâ”€â”€ SearchResults.tsx      # ê²€ìƒ‰ ê²°ê³¼ UI
â”‚   â””â”€â”€ index.ts
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ use-file-attachment.ts (ìœ„ hooks/ì—ì„œ ì´ë™)
â””â”€â”€ types/
    â””â”€â”€ index.ts (chat.tsì™€ search-engine.tsì—ì„œ re-export)
```

## ğŸ”§ êµ¬í˜„ ê³„íš

### Phase 1: Core File Store Module

#### 1.1 MCP Server ì¸í„°í˜ì´ìŠ¤ ì •ì˜

```typescript
import type { WebMCPServer, MCPTool } from '@/lib/mcp-types';

interface FileStoreServer extends WebMCPServer {
  name: 'file-store';
  version: '1.0.0';
  tools: [
    'createStore',
    'addContent', 
    'listContent',
    'readContent',
    'similaritySearch'
  ];
}
```

#### 1.2 MCP ë„êµ¬ ì •ì˜

```typescript
const tools: MCPTool[] = [
  {
    name: 'createStore',
    description: 'Create a new content store for file management',
    inputSchema: {
      type: 'object',
      properties: {
        metadata: {
          type: 'object',
          properties: {
            name: { type: 'string' },
            description: { type: 'string' },
            sessionId: { type: 'string' }
          }
        }
      }
    }
  },
  {
    name: 'addContent',
    description: 'Add file content with chunking and indexing',
    inputSchema: {
      type: 'object',
      properties: {
        storeId: { type: 'string' },
        content: { type: 'string' },
        metadata: {
          type: 'object',
          properties: {
            filename: { type: 'string' },
            mimeType: { type: 'string' },
            size: { type: 'number' },
            uploadedAt: { type: 'string' }
          },
          required: ['filename', 'mimeType', 'size', 'uploadedAt']
        }
      },
      required: ['storeId', 'content', 'metadata']
    }
  },
  {
    name: 'listContent',
    description: 'List content summaries in a store',
    inputSchema: {
      type: 'object',
      properties: {
        storeId: { type: 'string' },
        pagination: {
          type: 'object',
          properties: {
            offset: { type: 'number' },
            limit: { type: 'number' }
          }
        }
      },
      required: ['storeId']
    }
  },
  {
    name: 'readContent',
    description: 'Read specific line ranges from content',
    inputSchema: {
      type: 'object',
      properties: {
        storeId: { type: 'string' },
        contentId: { type: 'string' },
        lineRange: {
          type: 'object',
          properties: {
            fromLine: { type: 'number' },
            toLine: { type: 'number' }
          },
          required: ['fromLine']
        }
      },
      required: ['storeId', 'contentId', 'lineRange']
    }
  },
  {
    name: 'similaritySearch',
    description: 'Semantic search across content',
    inputSchema: {
      type: 'object',
      properties: {
        storeId: { type: 'string' },
        query: { type: 'string' },
        options: {
          type: 'object',
          properties: {
            topN: { type: 'number', default: 5 },
            threshold: { type: 'number', default: 0.5 }
          }
        }
      },
      required: ['storeId', 'query']
    }
  }
];
```

### Phase 2: ê²€ìƒ‰ ì—”ì§„ ì¶”ìƒí™” ë° BM25 êµ¬í˜„

#### 2.1 ê²€ìƒ‰ ì—”ì§„ ì¶”ìƒí™” ì¸í„°í˜ì´ìŠ¤

```typescript
// ê²€ìƒ‰ ì—”ì§„ ê³µí†µ ì¸í„°í˜ì´ìŠ¤
interface ISearchEngine {
  initialize(): Promise<void>;
  indexStore(storeId: string, chunks: ContentChunk[]): Promise<void>;
  search(storeId: string, query: string, options: SearchOptions): Promise<SearchResult[]>;
  isReady(): boolean;
  cleanup(): Promise<void>;
}

interface SearchOptions {
  topN: number;
  threshold?: number;
  searchType?: 'keyword' | 'semantic' | 'hybrid';
}

interface SearchResult {
  contentId: string;
  chunkId: string;
  context: string;
  lineRange: [number, number];
  score: number;
  relevanceType: 'keyword' | 'semantic';
}
```

#### 2.2 BM25 ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ (MVP)

**íŒŒì¼ ìœ„ì¹˜**: `src/lib/web-mcp/modules/file-store.ts`

```typescript
// ê¸°ì¡´ db.tsì˜ íƒ€ì…ë“¤ì„ import
import { dbService, dbUtils, FileStore, FileContent, FileChunk } from '@/lib/db';
// ê¸°ì¡´ chat.tsì˜ AttachmentReference í™œìš©
import { AttachmentReference } from '@/models/chat';
import BM25 from 'wink-bm25-text-search';
import { getLogger } from '@/lib/logger';

const logger = getLogger('BM25SearchEngine');

// ê²€ìƒ‰ ì—”ì§„ ê³µí†µ ì¸í„°í˜ì´ìŠ¤ (íŒŒì¼ ìƒë‹¨ì— ì •ì˜)
interface ISearchEngine {
  initialize(): Promise<void>;
  indexStore(storeId: string, chunks: FileChunk[]): Promise<void>;
  search(storeId: string, query: string, options: SearchOptions): Promise<SearchResult[]>;
  isReady(): boolean;
  cleanup(): Promise<void>;
}

interface SearchOptions {
  topN: number;
  threshold?: number;
  searchType?: 'keyword' | 'semantic' | 'hybrid';
}

interface SearchResult {
  contentId: string;
  chunkId: string;
  context: string;
  lineRange: [number, number];
  score: number;
  relevanceType: 'keyword' | 'semantic' | 'hybrid';
}

// BM25 ê²€ìƒ‰ ì—”ì§„ êµ¬í˜„ (ê¸°ì¡´ db.ts í™œìš©)
class BM25SearchEngine implements ISearchEngine {
  private bm25Indexes = new Map<string, any>(); // storeIdë³„ BM25 ì¸ë±ìŠ¤
  private chunkMappings = new Map<string, Map<string, FileChunk>>(); // ë¹ ë¥¸ ì¡°íšŒìš©
  private isInitialized = false;
  
  async initialize(): Promise<void> {
    if (this.isInitialized) return;
    
    logger.info('Initializing BM25 search engine');
    this.isInitialized = true;
  }
  
  async indexStore(storeId: string, chunks: FileChunk[]): Promise<void> {
    if (!this.isInitialized) {
      await this.initialize();
    }
    
    const bm25 = BM25();
    
    // BM25 ì„¤ì • ìµœì í™”
    bm25.defineConfig({
      fldWeights: { text: 1 },
      ovlpNormFactor: 0.5, // ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™”
      k1: 1.2,            // ë‹¨ì–´ ë¹ˆë„ í¬í™” ë§¤ê°œë³€ìˆ˜
      b: 0.75             // ë¬¸ì„œ ê¸¸ì´ ì •ê·œí™” ê°•ë„
    });
    
    // ì²­í¬ ë°ì´í„° ì¤€ë¹„ ë° ì¸ë±ì‹±
    const chunkMapping = new Map<string, FileChunk>();
    
    chunks.forEach(chunk => {
      // í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
      const processedText = this.preprocessText(chunk.text);
      
      bm25.addDoc({
        id: chunk.id, // ê¸°ì¡´ db.tsì˜ id í•„ë“œ ì‚¬ìš©
        text: processedText
      });
      
      chunkMapping.set(chunk.id, chunk);
    });
    
    // ì¸ë±ìŠ¤ í†µí•© ë° ì €ì¥
    bm25.consolidate();
    this.bm25Indexes.set(storeId, bm25);
    this.chunkMappings.set(storeId, chunkMapping);
    
    logger.info('BM25 index created', { 
      storeId, 
      chunkCount: chunks.length 
    });
  }
  
  async search(storeId: string, query: string, options: SearchOptions): Promise<SearchResult[]> {
    const bm25 = this.bm25Indexes.get(storeId);
    const chunkMapping = this.chunkMappings.get(storeId);
    
    if (!bm25 || !chunkMapping) {
      logger.warn('No index found for store', { storeId });
      return [];
    }
    
    // ì¿¼ë¦¬ ì „ì²˜ë¦¬
    const processedQuery = this.preprocessText(query);
    
    // BM25 ê²€ìƒ‰ ìˆ˜í–‰
    const searchResults = bm25.search(processedQuery);
    
    // ê²°ê³¼ ë³€í™˜ ë° ì •ë ¬
    const results: SearchResult[] = searchResults
      .slice(0, options.topN)
      .map((result: any) => {
        const chunk = chunkMapping.get(result.id);
        if (!chunk) return null;
        
        return {
          contentId: chunk.contentId,
          chunkId: chunk.id,
          context: chunk.text,
          lineRange: [chunk.startLine, chunk.endLine] as [number, number],
          score: result.score,
          relevanceType: 'keyword' as const
        };
      })
      .filter(Boolean) as SearchResult[];
    
    logger.debug('BM25 search completed', {
      storeId,
      query: processedQuery,
      resultCount: results.length
    });
    
    return results;
  }
  
  isReady(): boolean {
    return this.isInitialized;
  }
  
  async cleanup(): Promise<void> {
    this.bm25Indexes.clear();
    this.chunkMappings.clear();
    this.isInitialized = false;
    logger.info('BM25 search engine cleaned up');
  }
  
  // í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (BM25 ì„±ëŠ¥ í–¥ìƒ)
  private preprocessText(text: string): string {
    return text
      .toLowerCase()
      .replace(/[^\w\sê°€-í£]/g, ' ') // íŠ¹ìˆ˜ë¬¸ì ì œê±°, í•œê¸€ ìœ ì§€
      .replace(/\s+/g, ' ')          // ì—°ì† ê³µë°± ì •ë¦¬
      .trim();
  }
}
```

#### 2.3 ì„ë² ë”© ê²€ìƒ‰ ì—”ì§„ (í–¥í›„ í™•ì¥ìš©)

```typescript
class EmbeddingSearchEngine implements ISearchEngine {
  private embeddingService: EmbeddingService | null = null;
  private vectorSearch: VectorSearch | null = null;
  private isInitialized = false;
  
  async initialize(): Promise<void> {
    if (this.isInitialized) return;
    
    try {
      logger.info('Initializing embedding search engine...');
      
      this.embeddingService = new EmbeddingService();
      await this.embeddingService.initialize();
      
      this.vectorSearch = new VectorSearch();
      
      this.isInitialized = true;
      logger.info('Embedding search engine ready');
    } catch (error) {
      logger.error('Failed to initialize embedding search engine', { error });
      throw error;
    }
  }
  
  async indexStore(storeId: string, chunks: ContentChunk[]): Promise<void> {
    if (!this.isInitialized) {
      await this.initialize();
    }
    
    // ì„ë² ë”©ì´ ì—†ëŠ” ì²­í¬ë“¤ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±
    const chunksNeedingEmbedding = chunks.filter(chunk => !chunk.embedding);
    
    if (chunksNeedingEmbedding.length > 0) {
      logger.info('Generating embeddings for chunks', { 
        count: chunksNeedingEmbedding.length 
      });
      
      const texts = chunksNeedingEmbedding.map(chunk => chunk.text);
      const embeddings = await this.embeddingService!.generateBatchEmbeddings(texts);
      
      // ì„ë² ë”©ì„ ì²­í¬ì— ì €ì¥ (IndexedDB ì—…ë°ì´íŠ¸)
      for (let i = 0; i < chunksNeedingEmbedding.length; i++) {
        chunksNeedingEmbedding[i].embedding = embeddings[i];
        // TODO: IndexedDBì— ì„ë² ë”© ì—…ë°ì´íŠ¸
      }
    }
  }
  
  async search(storeId: string, query: string, options: SearchOptions): Promise<SearchResult[]> {
    if (!this.isInitialized || !this.embeddingService || !this.vectorSearch) {
      throw new Error('Embedding search engine not initialized');
    }
    
    // ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    const queryEmbedding = await this.embeddingService.generateEmbedding(query);
    
    // ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
    const results = await this.vectorSearch.search(
      queryEmbedding,
      storeId,
      { 
        topN: options.topN, 
        threshold: options.threshold || 0.5 
      },
      dbInstance! // TODO: DIë¡œ ê°œì„ 
    );
    
    return results.map(result => ({
      ...result,
      relevanceType: 'semantic' as const
    }));
  }
  
  isReady(): boolean {
    return this.isInitialized;
  }
  
  async cleanup(): Promise<void> {
    this.embeddingService = null;
    this.vectorSearch = null;
    this.isInitialized = false;
    logger.info('Embedding search engine cleaned up');
  }
}
```

#### 2.4 ì ì‘í˜• ê²€ìƒ‰ ë§¤ë‹ˆì € (Progressive Enhancement)

```typescript
class AdaptiveSearchManager {
  private bm25Engine: BM25SearchEngine;
  private embeddingEngine: EmbeddingSearchEngine | null = null;
  private currentStrategy: 'bm25' | 'embedding' | 'hybrid' = 'bm25';
  
  constructor() {
    this.bm25Engine = new BM25SearchEngine();
    this.initializeEnhancedSearch(); // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ˆê¸°í™”
  }
  
  async initialize(): Promise<void> {
    await this.bm25Engine.initialize();
    logger.info('Adaptive search manager initialized with BM25');
  }
  
  private async initializeEnhancedSearch(): Promise<void> {
    try {
      this.embeddingEngine = new EmbeddingSearchEngine();
      await this.embeddingEngine.initialize();
      
      // ì„ë² ë”© ì—”ì§„ì´ ì¤€ë¹„ë˜ë©´ ìë™ìœ¼ë¡œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œë¡œ ì „í™˜
      this.currentStrategy = 'hybrid';
      logger.info('Enhanced search capabilities enabled');
    } catch (error) {
      logger.warn('Enhanced search not available, using BM25 only', { error });
    }
  }
  
  async indexStore(storeId: string, chunks: ContentChunk[]): Promise<void> {
    // BM25ëŠ” í•­ìƒ ì¸ë±ì‹±
    await this.bm25Engine.indexStore(storeId, chunks);
    
    // ì„ë² ë”© ì—”ì§„ì´ ì¤€ë¹„ë˜ì—ˆë‹¤ë©´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¸ë±ì‹±
    if (this.embeddingEngine?.isReady()) {
      this.embeddingEngine.indexStore(storeId, chunks).catch(error => {
        logger.warn('Background embedding indexing failed', { error });
      });
    }
  }
  
  async search(storeId: string, query: string, options: SearchOptions): Promise<SearchResult[]> {
    switch (this.currentStrategy) {
      case 'bm25':
        return this.bm25Engine.search(storeId, query, options);
      
      case 'embedding':
        if (!this.embeddingEngine?.isReady()) {
          logger.warn('Embedding engine not ready, falling back to BM25');
          return this.bm25Engine.search(storeId, query, options);
        }
        return this.embeddingEngine.search(storeId, query, options);
      
      case 'hybrid':
        return this.hybridSearch(storeId, query, options);
      
      default:
        return this.bm25Engine.search(storeId, query, options);
    }
  }
  
  private async hybridSearch(storeId: string, query: string, options: SearchOptions): Promise<SearchResult[]> {
    const promises: Promise<SearchResult[]>[] = [
      this.bm25Engine.search(storeId, query, { ...options, topN: options.topN * 2 })
    ];
    
    if (this.embeddingEngine?.isReady()) {
      promises.push(
        this.embeddingEngine.search(storeId, query, { ...options, topN: options.topN * 2 })
      );
    }
    
    const [bm25Results, embeddingResults = []] = await Promise.all(promises);
    
    // ê²°ê³¼ ìœµí•© (ê°€ì¤‘ì¹˜ ê¸°ë°˜)
    return this.fuseResults(bm25Results, embeddingResults, options.topN);
  }
  
  private fuseResults(bm25Results: SearchResult[], embeddingResults: SearchResult[], topN: number): SearchResult[] {
    const resultMap = new Map<string, SearchResult>();
    
    // BM25 ê²°ê³¼ ì¶”ê°€ (ê°€ì¤‘ì¹˜: 0.4)
    bm25Results.forEach(result => {
      resultMap.set(result.chunkId, {
        ...result,
        score: result.score * 0.4
      });
    });
    
    // ì„ë² ë”© ê²°ê³¼ ì¶”ê°€ ë˜ëŠ” ì ìˆ˜ í•©ì‚° (ê°€ì¤‘ì¹˜: 0.6)
    embeddingResults.forEach(result => {
      const existing = resultMap.get(result.chunkId);
      if (existing) {
        existing.score += result.score * 0.6;
        existing.relevanceType = 'hybrid' as any;
      } else {
        resultMap.set(result.chunkId, {
          ...result,
          score: result.score * 0.6
        });
      }
    });
    
    // ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ Nê°œ ë°˜í™˜
    return Array.from(resultMap.values())
      .sort((a, b) => b.score - a.score)
      .slice(0, topN);
  }
  
  getSearchCapabilities(): {
    bm25Available: boolean;
    embeddingAvailable: boolean;
    currentStrategy: string;
  } {
    return {
      bm25Available: this.bm25Engine.isReady(),
      embeddingAvailable: this.embeddingEngine?.isReady() || false,
      currentStrategy: this.currentStrategy
    };
  }
  
  setSearchStrategy(strategy: 'bm25' | 'embedding' | 'hybrid'): void {
    this.currentStrategy = strategy;
    logger.info('Search strategy changed', { strategy });
  }
}
```

#### 2.1 ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì„¤ê³„

```typescript
interface Store {
  storeId: string;
  name: string;
  description?: string;
  sessionId?: string;
  createdAt: string;
  updatedAt: string;
}

interface Content {
  contentId: string;
  storeId: string;
  filename: string;
  mimeType: string;
  size: number;
  uploadedAt: string;
  content: string;
  lineCount: number;
  summary: string; // ì²« 10-20ì¤„
}

interface ContentChunk {
  chunkId: string;
  contentId: string;
  chunkIndex: number;
  text: string;
  startLine: number;
  endLine: number;
  embedding?: number[];
}
```

#### 2.2 IndexedDB ë˜í¼ í´ë˜ìŠ¤

```typescript
class FileStoreDB {
  private db: IDBDatabase | null = null;
  private readonly DB_NAME = 'FileAttachmentMCP';
  private readonly DB_VERSION = 1;
  
  async initialize(): Promise<void> {
    return new Promise((resolve, reject) => {
      const request = indexedDB.open(this.DB_NAME, this.DB_VERSION);
      
      request.onerror = () => reject(request.error);
      request.onsuccess = () => {
        this.db = request.result;
        resolve();
      };
      
      request.onupgradeneeded = (event) => {
        const db = (event.target as IDBOpenDBRequest).result;
        
        // Stores í…Œì´ë¸”
        if (!db.objectStoreNames.contains('stores')) {
          const storeOS = db.createObjectStore('stores', { keyPath: 'storeId' });
          storeOS.createIndex('sessionId', 'sessionId', { unique: false });
          storeOS.createIndex('createdAt', 'createdAt', { unique: false });
        }
        
        // Contents í…Œì´ë¸”
        if (!db.objectStoreNames.contains('contents')) {
          const contentOS = db.createObjectStore('contents', { keyPath: 'contentId' });
          contentOS.createIndex('storeId', 'storeId', { unique: false });
          contentOS.createIndex('filename', 'filename', { unique: false });
          contentOS.createIndex('uploadedAt', 'uploadedAt', { unique: false });
        }
        
        // ContentChunks í…Œì´ë¸”
        if (!db.objectStoreNames.contains('contentChunks')) {
          const chunkOS = db.createObjectStore('contentChunks', { keyPath: 'chunkId' });
          chunkOS.createIndex('contentId', 'contentId', { unique: false });
          chunkOS.createIndex('chunkIndex', 'chunkIndex', { unique: false });
        }
      };
    });
  }
  
  async createStore(store: Store): Promise<void> {
    const transaction = this.db!.transaction(['stores'], 'readwrite');
    const objectStore = transaction.objectStore('stores');
    await this.promisifyRequest(objectStore.add(store));
  }
  
  async addContent(content: Content): Promise<void> {
    const transaction = this.db!.transaction(['contents'], 'readwrite');
    const objectStore = transaction.objectStore('contents');
    await this.promisifyRequest(objectStore.add(content));
  }
  
  async addContentChunk(chunk: ContentChunk): Promise<void> {
    const transaction = this.db!.transaction(['contentChunks'], 'readwrite');
    const objectStore = transaction.objectStore('contentChunks');
    await this.promisifyRequest(objectStore.add(chunk));
  }
  
  private promisifyRequest<T>(request: IDBRequest<T>): Promise<T> {
    return new Promise((resolve, reject) => {
      request.onsuccess = () => resolve(request.result);
      request.onerror = () => reject(request.error);
    });
  }
}
```

### Phase 3: Embedding Service Integration

#### 3.1 transformers.js ê¸°ë°˜ ì„ë² ë”© ì„œë¹„ìŠ¤

```typescript
class EmbeddingService {
  private pipeline: any = null;
  private isInitialized = false;
  
  async initialize(): Promise<void> {
    if (this.isInitialized) return;
    
    try {
      // transformers.jsëŠ” Web Workerì—ì„œ ë™ì  import í•„ìš”
      const { pipeline } = await import('@huggingface/transformers');
      
      this.pipeline = await pipeline(
        'feature-extraction',
        'Xenova/all-MiniLM-L6-v2',
        {
          quantized: true, // ì„±ëŠ¥ ìµœì í™”
          device: 'webgpu', // WebGPU ì‚¬ìš© ê°€ëŠ¥ì‹œ
          progress_callback: (progress: any) => {
            console.log(`[Embedding] Model loading: ${Math.round(progress.progress * 100)}%`);
          }
        }
      );
      
      this.isInitialized = true;
    } catch (error) {
      console.error('[Embedding] Failed to initialize:', error);
      throw new Error(`Failed to initialize embedding service: ${error.message}`);
    }
  }
  
  async generateEmbedding(text: string): Promise<number[]> {
    if (!this.isInitialized || !this.pipeline) {
      throw new Error('Embedding service not initialized');
    }
    
    try {
      const result = await this.pipeline(text.trim());
      return Array.from(result.data);
    } catch (error) {
      console.error('[Embedding] Failed to generate embedding:', error);
      throw new Error(`Failed to generate embedding: ${error.message}`);
    }
  }
  
  async generateBatchEmbeddings(texts: string[]): Promise<number[][]> {
    const embeddings: number[][] = [];
    
    // ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œì–´
    const batchSize = 10;
    for (let i = 0; i < texts.length; i += batchSize) {
      const batch = texts.slice(i, i + batchSize);
      const batchEmbeddings = await Promise.all(
        batch.map(text => this.generateEmbedding(text))
      );
      embeddings.push(...batchEmbeddings);
    }
    
    return embeddings;
  }
}
```

#### 3.2 í…ìŠ¤íŠ¸ ì²­í‚¹ ì „ëµ

```typescript
class TextChunker {
  private readonly CHUNK_SIZE = 500; // ë¬¸ì ìˆ˜
  private readonly OVERLAP_SIZE = 50; // ê²¹ì¹˜ëŠ” ë¶€ë¶„
  
  chunkText(content: string): { text: string; startLine: number; endLine: number }[] {
    const lines = content.split('\n');
    const chunks: { text: string; startLine: number; endLine: number }[] = [];
    
    let currentChunk = '';
    let chunkStartLine = 0;
    let currentLine = 0;
    
    for (const line of lines) {
      currentChunk += line + '\n';
      currentLine++;
      
      // ì²­í¬ í¬ê¸° ì´ˆê³¼ì‹œ ë¶„í• 
      if (currentChunk.length >= this.CHUNK_SIZE) {
        chunks.push({
          text: currentChunk.trim(),
          startLine: chunkStartLine,
          endLine: currentLine - 1
        });
        
        // ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ëª‡ ì¤„ ìœ ì§€
        const overlapLines = this.getOverlapLines(currentChunk);
        currentChunk = overlapLines;
        chunkStartLine = Math.max(0, currentLine - this.getLineCount(overlapLines));
      }
    }
    
    // ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
    if (currentChunk.trim()) {
      chunks.push({
        text: currentChunk.trim(),
        startLine: chunkStartLine,
        endLine: currentLine - 1
      });
    }
    
    return chunks;
  }
  
  private getOverlapLines(chunk: string): string {
    const lines = chunk.split('\n');
    const overlapLineCount = Math.ceil(this.OVERLAP_SIZE / 50); // ëŒ€ëµì ì¸ ì¤„ ìˆ˜
    return lines.slice(-overlapLineCount).join('\n');
  }
  
  private getLineCount(text: string): number {
    return text.split('\n').length;
  }
}
```

### Phase 4: Vector Search Implementation

#### 4.1 ë¸Œë¼ìš°ì € ìµœì í™”ëœ ë²¡í„° ê²€ìƒ‰

```typescript
class VectorSearch {
  private readonly SIMILARITY_THRESHOLD = 0.3;
  
  // ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
  private cosineSimilarity(a: number[], b: number[]): number {
    if (a.length !== b.length) {
      throw new Error('Vectors must have the same length');
    }
    
    const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
    const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
    const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
    
    if (magnitudeA === 0 || magnitudeB === 0) return 0;
    
    return dotProduct / (magnitudeA * magnitudeB);
  }
  
  async search(
    queryEmbedding: number[],
    storeId: string,
    options: { topN: number; threshold: number },
    db: FileStoreDB
  ): Promise<SearchResult[]> {
    // IndexedDBì—ì„œ í•´ë‹¹ storeì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ
    const chunks = await this.getChunksByStore(storeId, db);
    
    // ì„ë² ë”©ì´ ì—†ëŠ” ì²­í¬ëŠ” ì œì™¸
    const chunksWithEmbeddings = chunks.filter(chunk => chunk.embedding);
    
    // ìœ ì‚¬ë„ ê³„ì‚° ë° ì •ë ¬
    const similarities = chunksWithEmbeddings.map(chunk => {
      const similarity = this.cosineSimilarity(queryEmbedding, chunk.embedding!);
      return {
        chunkId: chunk.chunkId,
        contentId: chunk.contentId,
        text: chunk.text,
        startLine: chunk.startLine,
        endLine: chunk.endLine,
        similarity
      };
    })
    .filter(item => item.similarity >= options.threshold)
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, options.topN);
    
    // ê²°ê³¼ í¬ë§·íŒ…
    return similarities.map(item => ({
      contentId: item.contentId,
      chunkId: item.chunkId,
      context: item.text,
      lineRange: [item.startLine, item.endLine],
      similarity: item.similarity
    }));
  }
  
  private async getChunksByStore(storeId: string, db: FileStoreDB): Promise<ContentChunk[]> {
    // ë¨¼ì € í•´ë‹¹ storeì˜ ëª¨ë“  content ID ì¡°íšŒ
    const contents = await db.getContentsByStore(storeId);
    const contentIds = contents.map(content => content.contentId);
    
    // ê° contentì˜ ì²­í¬ë“¤ ì¡°íšŒ
    const allChunks: ContentChunk[] = [];
    for (const contentId of contentIds) {
      const chunks = await db.getChunksByContent(contentId);
      allChunks.push(...chunks);
    }
    
    return allChunks;
  }
}
```

### Phase 5: Complete Module Implementation

#### 5.1 ë©”ì¸ íŒŒì¼ êµ¬í˜„ (file-store.ts)

```typescript
import type { WebMCPServer, MCPTool } from '@/lib/mcp-types';
import { AttachmentReference } from '@/models/chat'; // ê¸°ì¡´ íƒ€ì… í™œìš©
import { getLogger } from '@/lib/logger';

const logger = getLogger('FileStoreMCP');

// íƒ€ì… ì •ì˜ (ê¸°ì¡´ AttachmentReference í™œìš©)
interface CreateStoreInput {
  metadata?: {
    name?: string;
    description?: string;
    sessionId?: string;
  };
}

interface CreateStoreOutput {
  storeId: string;
  createdAt: string;
}

interface AddContentInput {
  storeId: string;
  content: string;
  metadata: {
    filename: string;
    mimeType: string;
    size: number;
    uploadedAt: string;
  };
}

// AttachmentReferenceë¥¼ í™œìš©í•œ ì¶œë ¥ íƒ€ì…
interface AddContentOutput extends Omit<AttachmentReference, 'storeId' | 'contentId'> {
  storeId: string;
  contentId: string;
  chunkCount: number; // ì¶”ê°€ ì •ë³´
}

// AttachmentReferenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì»¨í…ì¸  ìš”ì•½
type ContentSummary = AttachmentReference;

interface SearchResult {
  contentId: string;
  chunkId?: string;
  context: string;
  lineRange?: [number, number];
  similarity?: number;
}

// ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë“¤
let dbInstance: FileStoreDB | null = null;
let embeddingService: EmbeddingService | null = null;
let textChunker: TextChunker | null = null;
let vectorSearch: VectorSearch | null = null;

const fileStoreServer: WebMCPServer = {
  name: 'file-store',
  version: '1.0.0',
  description: 'File attachment and semantic search system using MCP protocol',
  
  async listTools(): Promise<MCPTool[]> {
    return tools;
  },
  
  async callTool(name: string, args: unknown): Promise<unknown> {
    logger.debug('File store tool called', { name, args });
    
    try {
      // í•„ìš”í•œ ì„œë¹„ìŠ¤ë“¤ ì´ˆê¸°í™”
      await this.initializeServices();
      
      switch (name) {
        case 'createStore':
          return await this.createStore(args as CreateStoreInput);
        
        case 'addContent':
          return await this.addContent(args as AddContentInput);
        
        case 'listContent':
          return await this.listContent(args as any);
        
        case 'readContent':
          return await this.readContent(args as any);
        
        case 'similaritySearch':
          return await this.similaritySearch(args as any);
        
        default:
          throw new Error(`Unknown tool: ${name}`);
      }
    } catch (error) {
      logger.error('File store tool execution failed', { name, error });
      throw error;
    }
  },
  
  async initializeServices(): Promise<void> {
    if (!dbInstance) {
      dbInstance = new FileStoreDB();
      await dbInstance.initialize();
    }
    
    if (!embeddingService) {
      embeddingService = new EmbeddingService();
      await embeddingService.initialize();
    }
    
    if (!textChunker) {
      textChunker = new TextChunker();
    }
    
    if (!vectorSearch) {
      vectorSearch = new VectorSearch();
    }
  },
  
  async createStore(input: CreateStoreInput): Promise<CreateStoreOutput> {
    const storeId = `store_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    const now = new Date().toISOString();
    
    const store: Store = {
      storeId,
      name: input.metadata?.name || 'Unnamed Store',
      description: input.metadata?.description,
      sessionId: input.metadata?.sessionId,
      createdAt: now,
      updatedAt: now
    };
    
    await dbInstance!.createStore(store);
    
    logger.info('Store created', { storeId, name: store.name });
    
    return {
      storeId,
      createdAt: now
    };
  },
  
  async addContent(input: AddContentInput): Promise<AddContentOutput> {
    const contentId = `content_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    const lines = input.content.split('\n');
    const summary = lines.slice(0, 20).join('\n'); // ì²« 20ì¤„ì„ ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©
    
    // Content ì €ì¥
    const content: Content = {
      contentId,
      storeId: input.storeId,
      filename: input.metadata.filename,
      mimeType: input.metadata.mimeType,
      size: input.metadata.size,
      uploadedAt: input.metadata.uploadedAt,
      content: input.content,
      lineCount: lines.length,
      summary
    };
    
    await dbInstance!.addContent(content);
    
    // í…ìŠ¤íŠ¸ ì²­í‚¹
    const chunks = textChunker!.chunkText(input.content);
    
    // ê° ì²­í¬ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± ë° ì €ì¥
    const chunkPromises = chunks.map(async (chunkData, index) => {
      const chunkId = `${contentId}_chunk_${index}`;
      
      try {
        const embedding = await embeddingService!.generateEmbedding(chunkData.text);
        
        const chunk: ContentChunk = {
          chunkId,
          contentId,
          chunkIndex: index,
          text: chunkData.text,
          startLine: chunkData.startLine,
          endLine: chunkData.endLine,
          embedding
        };
        
        await dbInstance!.addContentChunk(chunk);
      } catch (error) {
        logger.warn('Failed to generate embedding for chunk', { chunkId, error });
        
        // ì„ë² ë”© ìƒì„± ì‹¤íŒ¨í•´ë„ í…ìŠ¤íŠ¸ëŠ” ì €ì¥
        const chunk: ContentChunk = {
          chunkId,
          contentId,
          chunkIndex: index,
          text: chunkData.text,
          startLine: chunkData.startLine,
          endLine: chunkData.endLine
        };
        
        await dbInstance!.addContentChunk(chunk);
      }
    });
    
    await Promise.all(chunkPromises);
    
    logger.info('Content added with chunks', { 
      contentId, 
      filename: input.metadata.filename,
      chunkCount: chunks.length 
    });
    
    return {
      contentId,
      chunkCount: chunks.length
    };
  },
  
  async listContent(input: { storeId: string; pagination?: { offset?: number; limit?: number } }): Promise<{ contents: ContentSummary[] }> {
    const contents = await dbInstance!.getContentsByStore(input.storeId);
    
    // í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
    const offset = input.pagination?.offset || 0;
    const limit = input.pagination?.limit || 50;
    const paginatedContents = contents.slice(offset, offset + limit);
    
    const summaries: ContentSummary[] = paginatedContents.map(content => ({
      contentId: content.contentId,
      filename: content.filename,
      mimeType: content.mimeType,
      size: content.size,
      uploadedAt: content.uploadedAt,
      lineCount: content.lineCount,
      summary: content.summary
    }));
    
    return { contents: summaries };
  },
  
  async readContent(input: { 
    storeId: string; 
    contentId: string; 
    lineRange: { fromLine: number; toLine?: number } 
  }): Promise<{ content: string; lineRange: [number, number] }> {
    const content = await dbInstance!.getContentById(input.contentId);
    
    if (!content || content.storeId !== input.storeId) {
      throw new Error(`Content not found: ${input.contentId}`);
    }
    
    const lines = content.content.split('\n');
    const fromLine = Math.max(0, input.lineRange.fromLine - 1); // 1-based to 0-based
    const toLine = Math.min(lines.length - 1, (input.lineRange.toLine || lines.length) - 1);
    
    const selectedLines = lines.slice(fromLine, toLine + 1);
    
    return {
      content: selectedLines.join('\n'),
      lineRange: [fromLine + 1, toLine + 1] // 0-based to 1-based
    };
  },
  
  async similaritySearch(input: {
    storeId: string;
    query: string;
    options?: { topN?: number; threshold?: number }
  }): Promise<{ results: SearchResult[] }> {
    const options = {
      topN: input.options?.topN || 5,
      threshold: input.options?.threshold || 0.5
    };
    
    // ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    const queryEmbedding = await embeddingService!.generateEmbedding(input.query);
    
    // ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
    const results = await vectorSearch!.search(
      queryEmbedding,
      input.storeId,
      options,
      dbInstance!
    );
    
    logger.info('Similarity search completed', {
      storeId: input.storeId,
      query: input.query,
      resultCount: results.length
    });
    
    return { results };
  }
};

export default fileStoreServer;
```

## ğŸ”Œ WebMCPContext í†µí•©

### Provider ì„¤ì • ì—…ë°ì´íŠ¸

```typescript
// App.tsx ì—…ë°ì´íŠ¸
function App() {
  return (
    <WebMCPProvider
      workerPath="/workers/mcp-worker.js"
      servers={['calculator', 'filesystem', 'file-store']} // ğŸ†• ì¶”ê°€
      autoLoad={true}
    >
      <UnifiedMCPProvider>
        <YourApp />
      </UnifiedMCPProvider>
    </WebMCPProvider>
  );
}
```

### Hook ì‚¬ìš© ì˜ˆì‹œ (ê¸°ì¡´ AttachmentReference í™œìš©)

```typescript
// íŒŒì¼ ì²¨ë¶€ ì»´í¬ë„ŒíŠ¸ì—ì„œ ì‚¬ìš©
import { useWebMCPTools } from '@/hooks/use-web-mcp';
import { AttachmentReference } from '@/models/chat'; // ê¸°ì¡´ íƒ€ì… í™œìš©

function FileAttachmentComponent() {
  const { executeCall } = useWebMCPTools();
  
  const uploadFile = async (file: File): Promise<AttachmentReference> => {
    try {
      // 1. ìŠ¤í† ì–´ ìƒì„± (ì„¸ì…˜ë‹¹ í•œ ë²ˆ)
      const { storeId } = await executeCall('file-store', 'createStore', {
        metadata: { sessionId: 'current-session', name: 'Session Files' }
      });
      
      // 2. íŒŒì¼ ë‚´ìš© ì¶”ê°€ (AttachmentReference í˜•ì‹ìœ¼ë¡œ ë°˜í™˜)
      const content = await file.text();
      const result = await executeCall('file-store', 'addContent', {
        storeId,
        content,
        metadata: {
          filename: file.name,
          mimeType: file.type,
          size: file.size,
          uploadedAt: new Date().toISOString()
        }
      }) as AddContentOutput;
      
      // AttachmentReference í˜•ì‹ìœ¼ë¡œ ë³€í™˜
      const attachment: AttachmentReference = {
        storeId: result.storeId,
        contentId: result.contentId,
        filename: result.filename,
        mimeType: result.mimeType,
        size: result.size,
        lineCount: result.lineCount,
        preview: result.preview,
        uploadedAt: result.uploadedAt,
        chunkCount: result.chunkCount,
        lastAccessedAt: new Date().toISOString()
      };
      
      logger.info('File uploaded successfully', { 
        contentId: attachment.contentId, 
        filename: attachment.filename, 
        chunkCount: attachment.chunkCount 
      });
      
      return attachment;
    } catch (error) {
      logger.error('File upload failed', { filename: file.name, error });
      throw error;
    }
  };
  
  const searchContent = async (query: string, storeId: string) => {
    try {
      const { results } = await executeCall('file-store', 'similaritySearch', {
        storeId,
        query,
        options: { topN: 5, threshold: 0.5 }
      });
      
      return results;
    } catch (error) {
      logger.error('Content search failed', { query, error });
      throw error;
    }
  };
}
```

## ğŸ“¦ Dependencies ë° ì„¤ì •

### package.json ì—…ë°ì´íŠ¸ (BM25 ìš°ì„ )

```json
{
  "dependencies": {
    "wink-bm25-text-search": "^2.0.7"
  },
  "optionalDependencies": {
    "@huggingface/transformers": "^3.0.0"
  },
  "devDependencies": {
    "@types/web": "^0.0.99"
  }
}
```

### vite.config.ts ì—…ë°ì´íŠ¸

```typescript
export default defineConfig({
  // ...existing config
  optimizeDeps: {
    include: ['wink-bm25-text-search'],
    exclude: ['@huggingface/transformers'] // ì˜µì…˜ìœ¼ë¡œ ì œì™¸
  },
  worker: {
    format: 'es',
    plugins: () => [
      // transformers.js WASM ì§€ì› (í–¥í›„ í™•ì¥ìš©)
    ]
  },
  build: {
    rollupOptions: {
      output: {
        manualChunks: {
          'search-engines': ['wink-bm25-text-search'],
          'transformers': ['@huggingface/transformers'] // ì˜µì…˜
        }
      }
    }
  },
  server: {
    headers: {
      // transformers.js ì‚¬ìš© ì‹œ í•„ìš”í•œ í—¤ë” (í–¥í›„ í™•ì¥ìš©)
      'Cross-Origin-Embedder-Policy': 'require-corp',
      'Cross-Origin-Opener-Policy': 'same-origin'
    }
  }
});
```

### ì„¤ì • ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜

```typescript
interface FileStoreConfig {
  searchEngine: 'bm25' | 'embedding' | 'hybrid' | 'auto';
  enableProgressTracking: boolean;
  maxFileSize: number;
  chunkSize: number;
  memoryLimit: number;
  bm25Config?: {
    k1: number;
    b: number;
    ovlpNormFactor: number;
  };
  embeddingConfig?: {
    model: string;
    quantized: boolean;
    device: 'cpu' | 'webgpu';
  };
}

const DEFAULT_CONFIG: FileStoreConfig = {
  searchEngine: 'auto', // BM25ë¡œ ì‹œì‘, ì„ë² ë”© ì¤€ë¹„ë˜ë©´ í•˜ì´ë¸Œë¦¬ë“œë¡œ ì „í™˜
  enableProgressTracking: true,
  maxFileSize: 50 * 1024 * 1024, // 50MB
  chunkSize: 500,
  memoryLimit: 50 * 1024 * 1024, // 50MB
  bm25Config: {
    k1: 1.2,
    b: 0.75,
    ovlpNormFactor: 0.5
  },
  embeddingConfig: {
    model: 'Xenova/all-MiniLM-L6-v2',
    quantized: true,
    device: 'webgpu'
  }
};
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### BM25 ê¸°ë°˜ í†µí•© í…ŒìŠ¤íŠ¸

```typescript
export async function testFileStoreModuleBM25(): Promise<boolean> {
  const logger = getLogger('FileStoreTest');
  
  try {
    const proxy = createWebMCPProxy({ workerPath: '/workers/mcp-worker.js' });
    await proxy.initialize();
    
    // ì„œë²„ ë¡œë“œ
    await proxy.loadServer('file-store');
    
    // 1. ìŠ¤í† ì–´ ìƒì„± í…ŒìŠ¤íŠ¸
    const { storeId } = await proxy.callTool('file-store', 'createStore', {
      metadata: { 
        name: 'BM25 Test Store', 
        description: 'Testing BM25 search functionality' 
      }
    });
    
    logger.info('Store created successfully', { storeId });
    
    // 2. ë‹¤ì–‘í•œ ì»¨í…ì¸  ì¶”ê°€ (BM25 í…ŒìŠ¤íŠ¸ìš©)
    const testDocuments = [
      {
        filename: 'ml_basics.txt',
        content: `
          Machine learning is a subset of artificial intelligence (AI).
          It involves training algorithms on data to make predictions.
          Supervised learning uses labeled data for training.
          Unsupervised learning finds patterns in unlabeled data.
        `
      },
      {
        filename: 'deep_learning.txt', 
        content: `
          Deep learning uses neural networks with multiple layers.
          Convolutional neural networks are great for image processing.
          Recurrent neural networks handle sequential data well.
          Transformers have revolutionized natural language processing.
        `
      },
      {
        filename: 'nlp_guide.txt',
        content: `
          Natural language processing helps computers understand text.
          Tokenization breaks text into words or subwords.
          Named entity recognition identifies important entities.
          Sentiment analysis determines emotional tone of text.
        `
      }
    ];
    
    const contentIds: string[] = [];
    
    for (const doc of testDocuments) {
      const { contentId, chunkCount } = await proxy.callTool('file-store', 'addContent', {
        storeId,
        content: doc.content,
        metadata: {
          filename: doc.filename,
          mimeType: 'text/plain',
          size: doc.content.length,
          uploadedAt: new Date().toISOString()
        }
      });
      
      contentIds.push(contentId);
      logger.info('Content added', { filename: doc.filename, contentId, chunkCount });
    }
    
    // 3. BM25 ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ë‹¤ì–‘í•œ ì¿¼ë¦¬)
    const searchQueries = [
      { query: 'neural networks', expectedTerms: ['neural', 'networks', 'deep'] },
      { query: 'machine learning algorithms', expectedTerms: ['machine', 'learning', 'algorithms'] },
      { query: 'text processing NLP', expectedTerms: ['text', 'processing', 'nlp'] },
      { query: 'supervised unsupervised', expectedTerms: ['supervised', 'unsupervised'] }
    ];
    
    for (const { query, expectedTerms } of searchQueries) {
      const { results } = await proxy.callTool('file-store', 'similaritySearch', {
        storeId,
        query,
        options: { topN: 3, searchType: 'keyword' }
      });
      
      logger.info('BM25 search completed', { 
        query, 
        resultCount: results.length,
        topScore: results[0]?.score || 0
      });
      
      // ê²°ê³¼ ê²€ì¦: ê´€ë ¨ ìš©ì–´ê°€ í¬í•¨ëœ ì²­í¬ê°€ ìƒìœ„ì— ìˆëŠ”ì§€ í™•ì¸
      const hasRelevantResults = results.some(result => 
        expectedTerms.some(term => 
          result.context.toLowerCase().includes(term.toLowerCase())
        )
      );
      
      if (!hasRelevantResults && results.length > 0) {
        logger.warn('Search may not be working correctly', { query, results });
      }
    }
    
    // 4. ì—£ì§€ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
    const { results: emptyResults } = await proxy.callTool('file-store', 'similaritySearch', {
      storeId,
      query: 'nonexistent random terms xyz123',
      options: { topN: 5 }
    });
    
    logger.info('Empty query test', { 
      query: 'nonexistent terms',
      resultCount: emptyResults.length 
    });
    
    // 5. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ê²€ìƒ‰ ì†ë„)
    const startTime = Date.now();
    
    await Promise.all([
      proxy.callTool('file-store', 'similaritySearch', {
        storeId, query: 'machine learning', options: { topN: 5 }
      }),
      proxy.callTool('file-store', 'similaritySearch', {
        storeId, query: 'deep networks', options: { topN: 5 }
      }),
      proxy.callTool('file-store', 'similaritySearch', {
        storeId, query: 'text processing', options: { topN: 5 }
      })
    ]);
    
    const searchTime = Date.now() - startTime;
    logger.info('Concurrent search performance', { searchTime, searchCount: 3 });
    
    // ê²€ì¦: ê¸°ë³¸ ê¸°ëŠ¥ì´ ëª¨ë‘ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸
    const allTestsPassed = 
      storeId && 
      contentIds.length === testDocuments.length && 
      searchTime < 1000; // 3ê°œ ê²€ìƒ‰ì´ 1ì´ˆ ì´ë‚´
    
    logger.info('BM25 tests completed', { 
      success: allTestsPassed,
      searchTime,
      contentCount: contentIds.length
    });
    
    return allTestsPassed;
  } catch (error) {
    logger.error('BM25 file store test failed', error);
    return false;
  }
}

// Progressive Enhancement í…ŒìŠ¤íŠ¸
export async function testProgressiveEnhancement(): Promise<boolean> {
  const logger = getLogger('ProgressiveTest');
  
  try {
    const proxy = createWebMCPProxy({ workerPath: '/workers/mcp-worker.js' });
    await proxy.initialize();
    await proxy.loadServer('file-store');
    
    // ê²€ìƒ‰ ì—”ì§„ ìƒíƒœ í™•ì¸
    const capabilities = await proxy.callTool('file-store', 'getSearchCapabilities', {});
    
    logger.info('Search capabilities', capabilities);
    
    // BM25ëŠ” í•­ìƒ ì‚¬ìš© ê°€ëŠ¥í•´ì•¼ í•¨
    if (!capabilities.bm25Available) {
      throw new Error('BM25 search engine should always be available');
    }
    
    // ì„ë² ë”© ì—”ì§„ì€ ì„ íƒì 
    if (capabilities.embeddingAvailable) {
      logger.info('Enhanced search features available');
      
      // í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
      const { results } = await proxy.callTool('file-store', 'similaritySearch', {
        storeId: 'test-store',
        query: 'test query',
        options: { searchType: 'hybrid' }
      });
      
      logger.info('Hybrid search test completed', { resultCount: results.length });
    } else {
      logger.info('Using BM25-only mode (normal for initial load)');
    }
    
    return true;
  } catch (error) {
    logger.error('Progressive enhancement test failed', error);
    return false;
  }
}
```

## ğŸš€ ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```typescript
class MemoryManager {
  private readonly MAX_CHUNKS_IN_MEMORY = 1000;
  private readonly MAX_EMBEDDINGS_IN_MEMORY = 500;
  private chunkCache = new Map<string, ContentChunk>();
  private embeddingCache = new Map<string, number[]>();
  
  manageMemory(): void {
    if (this.chunkCache.size > this.MAX_CHUNKS_IN_MEMORY) {
      this.evictOldestChunks();
    }
    
    if (this.embeddingCache.size > this.MAX_EMBEDDINGS_IN_MEMORY) {
      this.evictOldestEmbeddings();
    }
  }
  
  private evictOldestChunks(): void {
    const entries = Array.from(this.chunkCache.entries());
    const toEvict = entries.slice(0, entries.length - this.MAX_CHUNKS_IN_MEMORY + 100);
    
    for (const [key] of toEvict) {
      this.chunkCache.delete(key);
    }
  }
  
  private evictOldestEmbeddings(): void {
    const entries = Array.from(this.embeddingCache.entries());
    const toEvict = entries.slice(0, entries.length - this.MAX_EMBEDDINGS_IN_MEMORY + 50);
    
    for (const [key] of toEvict) {
      this.embeddingCache.delete(key);
    }
  }
  
  getMemoryUsage(): { chunkCount: number; embeddingCount: number } {
    return {
      chunkCount: this.chunkCache.size,
      embeddingCount: this.embeddingCache.size
    };
  }
}
```

### ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

```typescript
class BatchProcessor {
  private readonly BATCH_SIZE = 10;
  private readonly BATCH_DELAY = 100; // ms
  
  async processBatch<T, R>(
    items: T[],
    processor: (item: T) => Promise<R>
  ): Promise<R[]> {
    const results: R[] = [];
    
    for (let i = 0; i < items.length; i += this.BATCH_SIZE) {
      const batch = items.slice(i, i + this.BATCH_SIZE);
      const batchResults = await Promise.all(
        batch.map(item => processor(item))
      );
      
      results.push(...batchResults);
      
      // ë‹¤ìŒ ë°°ì¹˜ ì²˜ë¦¬ ì „ ì§€ì—°
      if (i + this.BATCH_SIZE < items.length) {
        await this.delay(this.BATCH_DELAY);
      }
    }
    
    return results;
  }
  
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
```

## ğŸ“ˆ ì„±ê³µ ë©”íŠ¸ë¦­ìŠ¤ ë° ëª¨ë‹ˆí„°ë§

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ìŠ¤

- **âœ… íŒŒì¼ ì²˜ë¦¬**: 10MB íŒŒì¼ì„ 5ì´ˆ ì´ë‚´ ì²˜ë¦¬
- **âœ… ê²€ìƒ‰ ì„±ëŠ¥**: 1000ê°œ ì²­í¬ì—ì„œ 100ms ì´ë‚´ ê²€ìƒ‰
- **âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©**: ë¸Œë¼ìš°ì € 100MB ì´ë‚´ ìœ ì§€
- **âœ… ì •í™•ë„**: ì˜ë¯¸ì  ê²€ìƒ‰ ì •í™•ë„ 80% ì´ìƒ

### ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```typescript
interface PerformanceMetrics {
  totalStores: number;
  totalContents: number;
  totalChunks: number;
  averageSearchTime: number;
  memoryUsage: number;
  embeddingGenerationTime: number;
  indexingTime: number;
}

class MetricsCollector {
  private metrics: PerformanceMetrics = {
    totalStores: 0,
    totalContents: 0,
    totalChunks: 0,
    averageSearchTime: 0,
    memoryUsage: 0,
    embeddingGenerationTime: 0,
    indexingTime: 0
  };
  
  updateMetrics(update: Partial<PerformanceMetrics>): void {
    this.metrics = { ...this.metrics, ...update };
  }
  
  getMetrics(): PerformanceMetrics {
    return { ...this.metrics };
  }
  
  logMetrics(): void {
    console.table(this.metrics);
  }
}
```

## ğŸ¯ ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„ (ê¸°ì¡´ íƒ€ì… í™œìš©)

ì´ êµ¬í˜„ ê³„íšì„ í†µí•´ Web Worker MCP ëª¨ë“ˆë¡œ ì™„ì „í•œ íŒŒì¼ ì²¨ë¶€ ë° ì˜ë¯¸ì  ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì£¼ìš” ì¥ì 

1. **ê¸°ì¡´ íƒ€ì… ì¬ì‚¬ìš©**: `AttachmentReference`ê°€ ì´ë¯¸ ì™„ë²½í•˜ê²Œ ì •ì˜ë˜ì–´ ì¼ê´€ì„± í™•ë³´
2. **ë¸Œë¼ìš°ì € ë„¤ì´í‹°ë¸Œ**: ì™¸ë¶€ ì„œë²„ ì˜ì¡´ì„± ì—†ì´ ë¸Œë¼ìš°ì €ì—ì„œ ì™„ì „ ë™ì‘
3. **í‘œì¤€ MCP ì¸í„°í˜ì´ìŠ¤**: ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì™„ë²½ í†µí•©
4. **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ì´ë‚˜ ì„ë² ë”© ëª¨ë¸ ì‰½ê²Œ êµì²´
5. **ì„±ëŠ¥ ìµœì í™”**: ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì•ˆì •ì  ë™ì‘

### ğŸ”— ê¸°ì¡´ íƒ€ì… í†µí•© ì´ì 

```typescript
// src/models/chat.tsì˜ AttachmentReferenceë¥¼ ê·¸ëŒ€ë¡œ í™œìš©
export interface Message {
  // ...ê¸°ì¡´ í•„ë“œë“¤
  attachments?: AttachmentReference[]; // âœ… ì´ë¯¸ ì •ì˜ë¨!
}

// ìƒˆë¡œìš´ íŒŒì¼ ì²¨ë¶€ ê¸°ëŠ¥ì´ ê¸°ì¡´ ì±„íŒ… ì‹œìŠ¤í…œê³¼ ì™„ë²½ í˜¸í™˜
const message: Message = {
  id: 'msg-1',
  content: 'íŒŒì¼ì„ ì²¨ë¶€í–ˆìŠµë‹ˆë‹¤.',
  attachments: [uploadedAttachment], // AttachmentReference ê·¸ëŒ€ë¡œ ì‚¬ìš©
  // ...
};
```

### MVP ë‹¨ê³„ë³„ êµ¬í˜„ ìš°ì„ ìˆœìœ„

```text
ğŸ¯ MVP Phase 1a: BM25 ê¸°ë°˜ ê²€ìƒ‰ (ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥)
â”œâ”€â”€ âœ… ê¸°ë³¸ íŒŒì¼ ì €ì¥ (IndexedDB)
â”œâ”€â”€ âœ… í…ìŠ¤íŠ¸ ì²­í‚¹
â”œâ”€â”€ âœ… BM25 ì¸ë±ì‹± ë° ê²€ìƒ‰
â”œâ”€â”€ âœ… í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
â””â”€â”€ âœ… ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬

ğŸš€ MVP Phase 1b: UI í†µí•© ë° ì‚¬ìš©ì„± ê°œì„ 
â”œâ”€â”€ ğŸ“ íŒŒì¼ ì—…ë¡œë“œ ì»´í¬ë„ŒíŠ¸
â”œâ”€â”€ ğŸ” ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ UI
â”œâ”€â”€ ğŸ“Š ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§
â”œâ”€â”€ âš ï¸  ì—ëŸ¬ ì²˜ë¦¬ ë° ì‚¬ìš©ì í”¼ë“œë°±
â””â”€â”€ ğŸ§ª ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

ğŸ”¬ Enhanced Phase 2a: ì„ë² ë”© ê²€ìƒ‰ (ì„ íƒì  ë¡œë”©)
â”œâ”€â”€ ğŸ¤– transformers.js ë°±ê·¸ë¼ìš´ë“œ ë¡œë”©
â”œâ”€â”€ ğŸ§  ì„ë² ë”© ìƒì„± ë° ì €ì¥
â”œâ”€â”€ ğŸ”„ BM25 â†’ ì„ë² ë”© ì ì§„ì  ì „í™˜
â””â”€â”€ ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

âš¡ Enhanced Phase 2b: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™”
â”œâ”€â”€ ğŸ”€ BM25 + ì„ë² ë”© ê²°ê³¼ ìœµí•©
â”œâ”€â”€ âš™ï¸  ì‚¬ìš©ì ì„¤ì • ê²€ìƒ‰ ì „ëµ
â”œâ”€â”€ ğŸ›ï¸  ê°€ì¤‘ì¹˜ ì¡°ì • ë° íŠœë‹
â””â”€â”€ ğŸ† ìµœì  ê²€ìƒ‰ ê²°ê³¼ ì œê³µ

ğŸŒŸ Advanced Phase 3: í™•ì¥ ê¸°ëŠ¥
â”œâ”€â”€ ğŸ“„ ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›
â”œâ”€â”€ ğŸ·ï¸  ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§
â”œâ”€â”€ ğŸ’¾ ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
â””â”€â”€ ğŸ”— ê²€ìƒ‰ ê²°ê³¼ ì—°ê´€ì„± ë¶„ì„
```

### ë‹¤ìŒ ë‹¨ê³„ (ê¸°ì¡´ íƒ€ì… ê¸°ë°˜)

1. **Phase 1a êµ¬í˜„**: ê²€ìƒ‰ ì—”ì§„ íƒ€ì… ì •ì˜ ë° BM25 Core Module
   - `src/models/search-engine.ts` ìƒì„± (ISearchEngine, SearchOptions, SearchResult)
   - `wink-bm25-text-search` í†µí•© ë° BM25SearchEngine êµ¬í˜„
   - ê¸°ì¡´ `AttachmentReference` íƒ€ì…ê³¼ ì—°ë™í•˜ëŠ” MCP ë„êµ¬ êµ¬í˜„

2. **Phase 1b êµ¬í˜„**: UI í†µí•© ë° ê¸°ì¡´ ì±„íŒ… ì‹œìŠ¤í…œê³¼ ì—°ê²°
   - WebMCPContextì— íŒŒì¼ ì²¨ë¶€ ê¸°ëŠ¥ ì¶”ê°€
   - `AttachmentReference[]`ë¥¼ í™œìš©í•œ íŒŒì¼ ì—…ë¡œë“œ/ê²€ìƒ‰ ì»´í¬ë„ŒíŠ¸
   - `Message.attachments` í•„ë“œì™€ ì™„ì „ í˜¸í™˜ë˜ëŠ” UI êµ¬í˜„

3. **Phase 2a êµ¬í˜„**: ì„ë² ë”© ê¸°ëŠ¥ ì ì§„ì  ì¶”ê°€
   - transformers.js ì„ íƒì  ë¡œë”©
   - ë°±ê·¸ë¼ìš´ë“œ ì„ë² ë”© ìƒì„± ë° ê¸°ì¡´ `FileChunk` í…Œì´ë¸”ê³¼ ì—°ë™
   - Progressive Enhancement ì ìš©

4. **Phase 2b ìµœì í™”**: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ì±„íŒ… í†µí•©
   - BM25 + ì„ë² ë”© ê²°ê³¼ ìœµí•© ì•Œê³ ë¦¬ì¦˜
   - ì±„íŒ… ì»¨í…ìŠ¤íŠ¸ì— íŒŒì¼ ì²¨ë¶€ ë‚´ìš© ìë™ í¬í•¨
   - `AttachmentReference` ê¸°ë°˜ íŒŒì¼ ê´€ë¦¬ ë° ê²€ìƒ‰

5. **Phase 3 í™•ì¥**: ê¸°ì¡´ ì‹œìŠ¤í…œ ì™„ì „ í†µí•©
   - PDF, DOCX ë“± ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›
   - ì„¸ì…˜ë³„ íŒŒì¼ ê´€ë¦¬ (Session í…Œì´ë¸”ê³¼ ì—°ë™)
   - Assistantë³„ íŒŒì¼ ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬

### ğŸš€ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•œ ê¸°ì¡´ ìì‚°

- âœ… `AttachmentReference` ì¸í„°í˜ì´ìŠ¤ (ì™„ë²½í•˜ê²Œ ì •ì˜ë¨)
- âœ… `Message.attachments` í•„ë“œ (ì±„íŒ…ê³¼ ì¦‰ì‹œ ì—°ë™ ê°€ëŠ¥)
- âœ… ê¸°ì¡´ db.tsì˜ CRUD íŒ¨í„´ (íŒŒì¼ ì €ì¥ ë¡œì§ í™•ì¥ë¨)
- âœ… WebMCPContext êµ¬ì¡° (calculator/filesystem íŒ¨í„´ ì°¸ê³ )
- âœ… ë¡œê¹… ì‹œìŠ¤í…œ (`@/lib/logger`) ë° shadcn/ui ì»´í¬ë„ŒíŠ¸
